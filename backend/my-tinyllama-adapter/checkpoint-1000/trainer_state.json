{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.717391304347826,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02717391304347826,
      "grad_norm": 1.858280062675476,
      "learning_rate": 0.00019978260869565217,
      "loss": 2.9263,
      "step": 10
    },
    {
      "epoch": 0.05434782608695652,
      "grad_norm": 3.812354803085327,
      "learning_rate": 0.0001995380434782609,
      "loss": 3.0221,
      "step": 20
    },
    {
      "epoch": 0.08152173913043478,
      "grad_norm": 3.969658613204956,
      "learning_rate": 0.0001992663043478261,
      "loss": 2.4169,
      "step": 30
    },
    {
      "epoch": 0.10869565217391304,
      "grad_norm": 6.3559889793396,
      "learning_rate": 0.0001989945652173913,
      "loss": 2.0587,
      "step": 40
    },
    {
      "epoch": 0.1358695652173913,
      "grad_norm": 4.14518928527832,
      "learning_rate": 0.00019880434782608696,
      "loss": 1.8471,
      "step": 50
    },
    {
      "epoch": 0.16304347826086957,
      "grad_norm": 6.662506580352783,
      "learning_rate": 0.00019853260869565217,
      "loss": 2.1131,
      "step": 60
    },
    {
      "epoch": 0.19021739130434784,
      "grad_norm": 4.655934810638428,
      "learning_rate": 0.0001982608695652174,
      "loss": 1.6704,
      "step": 70
    },
    {
      "epoch": 0.21739130434782608,
      "grad_norm": 9.094929695129395,
      "learning_rate": 0.0001979891304347826,
      "loss": 1.6821,
      "step": 80
    },
    {
      "epoch": 0.24456521739130435,
      "grad_norm": 4.5194172859191895,
      "learning_rate": 0.00019771739130434784,
      "loss": 1.5413,
      "step": 90
    },
    {
      "epoch": 0.2717391304347826,
      "grad_norm": 14.408817291259766,
      "learning_rate": 0.00019744565217391305,
      "loss": 1.574,
      "step": 100
    },
    {
      "epoch": 0.29891304347826086,
      "grad_norm": 7.112241744995117,
      "learning_rate": 0.00019717391304347828,
      "loss": 1.3409,
      "step": 110
    },
    {
      "epoch": 0.32608695652173914,
      "grad_norm": 7.8925700187683105,
      "learning_rate": 0.0001969021739130435,
      "loss": 1.5467,
      "step": 120
    },
    {
      "epoch": 0.3532608695652174,
      "grad_norm": 6.134781360626221,
      "learning_rate": 0.0001966304347826087,
      "loss": 1.6731,
      "step": 130
    },
    {
      "epoch": 0.3804347826086957,
      "grad_norm": 10.355749130249023,
      "learning_rate": 0.00019635869565217393,
      "loss": 1.2999,
      "step": 140
    },
    {
      "epoch": 0.4076086956521739,
      "grad_norm": 4.096418380737305,
      "learning_rate": 0.00019608695652173914,
      "loss": 1.5674,
      "step": 150
    },
    {
      "epoch": 0.43478260869565216,
      "grad_norm": 4.768747806549072,
      "learning_rate": 0.00019581521739130435,
      "loss": 1.1745,
      "step": 160
    },
    {
      "epoch": 0.46195652173913043,
      "grad_norm": 6.453281879425049,
      "learning_rate": 0.00019554347826086958,
      "loss": 1.3976,
      "step": 170
    },
    {
      "epoch": 0.4891304347826087,
      "grad_norm": 4.45206356048584,
      "learning_rate": 0.0001952717391304348,
      "loss": 1.5724,
      "step": 180
    },
    {
      "epoch": 0.5163043478260869,
      "grad_norm": 5.2901411056518555,
      "learning_rate": 0.000195,
      "loss": 1.5382,
      "step": 190
    },
    {
      "epoch": 0.5434782608695652,
      "grad_norm": 4.825442314147949,
      "learning_rate": 0.00019472826086956523,
      "loss": 1.4633,
      "step": 200
    },
    {
      "epoch": 0.5706521739130435,
      "grad_norm": 6.178434371948242,
      "learning_rate": 0.00019445652173913044,
      "loss": 1.4216,
      "step": 210
    },
    {
      "epoch": 0.5978260869565217,
      "grad_norm": 7.09343957901001,
      "learning_rate": 0.00019418478260869567,
      "loss": 1.4035,
      "step": 220
    },
    {
      "epoch": 0.625,
      "grad_norm": 7.159855842590332,
      "learning_rate": 0.00019391304347826088,
      "loss": 1.4828,
      "step": 230
    },
    {
      "epoch": 0.6521739130434783,
      "grad_norm": 5.902719974517822,
      "learning_rate": 0.00019364130434782611,
      "loss": 1.2125,
      "step": 240
    },
    {
      "epoch": 0.6793478260869565,
      "grad_norm": 4.84725284576416,
      "learning_rate": 0.00019336956521739132,
      "loss": 1.4477,
      "step": 250
    },
    {
      "epoch": 0.7065217391304348,
      "grad_norm": 4.909605026245117,
      "learning_rate": 0.00019309782608695653,
      "loss": 1.217,
      "step": 260
    },
    {
      "epoch": 0.7336956521739131,
      "grad_norm": 6.655997276306152,
      "learning_rate": 0.00019282608695652176,
      "loss": 1.297,
      "step": 270
    },
    {
      "epoch": 0.7608695652173914,
      "grad_norm": 4.5640764236450195,
      "learning_rate": 0.00019255434782608697,
      "loss": 1.4114,
      "step": 280
    },
    {
      "epoch": 0.7880434782608695,
      "grad_norm": 4.893315315246582,
      "learning_rate": 0.00019228260869565218,
      "loss": 1.086,
      "step": 290
    },
    {
      "epoch": 0.8152173913043478,
      "grad_norm": 9.92303466796875,
      "learning_rate": 0.0001920108695652174,
      "loss": 1.4338,
      "step": 300
    },
    {
      "epoch": 0.842391304347826,
      "grad_norm": 7.21193265914917,
      "learning_rate": 0.00019173913043478262,
      "loss": 1.2129,
      "step": 310
    },
    {
      "epoch": 0.8695652173913043,
      "grad_norm": 7.024837970733643,
      "learning_rate": 0.00019146739130434783,
      "loss": 1.2183,
      "step": 320
    },
    {
      "epoch": 0.8967391304347826,
      "grad_norm": 6.16318941116333,
      "learning_rate": 0.00019119565217391303,
      "loss": 1.0701,
      "step": 330
    },
    {
      "epoch": 0.9239130434782609,
      "grad_norm": 6.199466228485107,
      "learning_rate": 0.00019092391304347827,
      "loss": 1.2121,
      "step": 340
    },
    {
      "epoch": 0.9510869565217391,
      "grad_norm": 5.623912811279297,
      "learning_rate": 0.00019065217391304348,
      "loss": 1.3395,
      "step": 350
    },
    {
      "epoch": 0.9782608695652174,
      "grad_norm": 6.265106201171875,
      "learning_rate": 0.0001903804347826087,
      "loss": 1.2074,
      "step": 360
    },
    {
      "epoch": 1.0054347826086956,
      "grad_norm": 7.836062431335449,
      "learning_rate": 0.00019010869565217392,
      "loss": 1.1569,
      "step": 370
    },
    {
      "epoch": 1.0326086956521738,
      "grad_norm": 6.503505229949951,
      "learning_rate": 0.00018983695652173915,
      "loss": 0.9372,
      "step": 380
    },
    {
      "epoch": 1.059782608695652,
      "grad_norm": 6.2892985343933105,
      "learning_rate": 0.00018956521739130436,
      "loss": 1.1895,
      "step": 390
    },
    {
      "epoch": 1.0869565217391304,
      "grad_norm": 7.21104097366333,
      "learning_rate": 0.0001892934782608696,
      "loss": 1.4854,
      "step": 400
    },
    {
      "epoch": 1.1141304347826086,
      "grad_norm": 7.502607345581055,
      "learning_rate": 0.0001890217391304348,
      "loss": 1.0431,
      "step": 410
    },
    {
      "epoch": 1.141304347826087,
      "grad_norm": 5.04808235168457,
      "learning_rate": 0.00018875,
      "loss": 1.1058,
      "step": 420
    },
    {
      "epoch": 1.1684782608695652,
      "grad_norm": 4.976306915283203,
      "learning_rate": 0.00018847826086956524,
      "loss": 0.9103,
      "step": 430
    },
    {
      "epoch": 1.1956521739130435,
      "grad_norm": 8.628564834594727,
      "learning_rate": 0.00018820652173913045,
      "loss": 1.1287,
      "step": 440
    },
    {
      "epoch": 1.2228260869565217,
      "grad_norm": 16.884870529174805,
      "learning_rate": 0.00018793478260869566,
      "loss": 0.9912,
      "step": 450
    },
    {
      "epoch": 1.25,
      "grad_norm": 5.307351589202881,
      "learning_rate": 0.00018766304347826086,
      "loss": 0.9098,
      "step": 460
    },
    {
      "epoch": 1.2771739130434783,
      "grad_norm": 8.836557388305664,
      "learning_rate": 0.0001873913043478261,
      "loss": 0.8656,
      "step": 470
    },
    {
      "epoch": 1.3043478260869565,
      "grad_norm": 8.308188438415527,
      "learning_rate": 0.0001871195652173913,
      "loss": 1.3327,
      "step": 480
    },
    {
      "epoch": 1.3315217391304348,
      "grad_norm": 6.589860916137695,
      "learning_rate": 0.0001868478260869565,
      "loss": 0.8747,
      "step": 490
    },
    {
      "epoch": 1.358695652173913,
      "grad_norm": 3.953298807144165,
      "learning_rate": 0.00018657608695652175,
      "loss": 0.8456,
      "step": 500
    },
    {
      "epoch": 1.3858695652173914,
      "grad_norm": 11.325197219848633,
      "learning_rate": 0.00018630434782608695,
      "loss": 0.9837,
      "step": 510
    },
    {
      "epoch": 1.4130434782608696,
      "grad_norm": 7.573903560638428,
      "learning_rate": 0.0001860326086956522,
      "loss": 1.257,
      "step": 520
    },
    {
      "epoch": 1.440217391304348,
      "grad_norm": 6.222319602966309,
      "learning_rate": 0.00018576086956521742,
      "loss": 0.8892,
      "step": 530
    },
    {
      "epoch": 1.4673913043478262,
      "grad_norm": 11.877246856689453,
      "learning_rate": 0.00018548913043478263,
      "loss": 1.2208,
      "step": 540
    },
    {
      "epoch": 1.4945652173913042,
      "grad_norm": 4.6044158935546875,
      "learning_rate": 0.00018521739130434784,
      "loss": 1.1729,
      "step": 550
    },
    {
      "epoch": 1.5217391304347827,
      "grad_norm": 9.579429626464844,
      "learning_rate": 0.00018494565217391304,
      "loss": 1.1511,
      "step": 560
    },
    {
      "epoch": 1.5489130434782608,
      "grad_norm": 11.49062442779541,
      "learning_rate": 0.00018467391304347828,
      "loss": 0.9277,
      "step": 570
    },
    {
      "epoch": 1.5760869565217392,
      "grad_norm": 7.303023815155029,
      "learning_rate": 0.00018440217391304349,
      "loss": 0.9939,
      "step": 580
    },
    {
      "epoch": 1.6032608695652173,
      "grad_norm": 9.111318588256836,
      "learning_rate": 0.0001841304347826087,
      "loss": 1.0109,
      "step": 590
    },
    {
      "epoch": 1.6304347826086958,
      "grad_norm": 8.42862606048584,
      "learning_rate": 0.00018385869565217393,
      "loss": 0.8483,
      "step": 600
    },
    {
      "epoch": 1.6576086956521738,
      "grad_norm": 4.577413558959961,
      "learning_rate": 0.00018358695652173913,
      "loss": 1.0883,
      "step": 610
    },
    {
      "epoch": 1.6847826086956523,
      "grad_norm": 5.787291526794434,
      "learning_rate": 0.00018331521739130434,
      "loss": 0.7834,
      "step": 620
    },
    {
      "epoch": 1.7119565217391304,
      "grad_norm": 6.509949207305908,
      "learning_rate": 0.00018304347826086958,
      "loss": 0.9003,
      "step": 630
    },
    {
      "epoch": 1.7391304347826086,
      "grad_norm": 4.616988182067871,
      "learning_rate": 0.00018277173913043478,
      "loss": 1.0329,
      "step": 640
    },
    {
      "epoch": 1.766304347826087,
      "grad_norm": 9.458673477172852,
      "learning_rate": 0.0001825,
      "loss": 0.8337,
      "step": 650
    },
    {
      "epoch": 1.7934782608695652,
      "grad_norm": 8.935589790344238,
      "learning_rate": 0.00018222826086956523,
      "loss": 0.707,
      "step": 660
    },
    {
      "epoch": 1.8206521739130435,
      "grad_norm": 7.959635257720947,
      "learning_rate": 0.00018195652173913046,
      "loss": 1.1736,
      "step": 670
    },
    {
      "epoch": 1.8478260869565217,
      "grad_norm": 8.486064910888672,
      "learning_rate": 0.00018168478260869567,
      "loss": 1.3442,
      "step": 680
    },
    {
      "epoch": 1.875,
      "grad_norm": 5.481043815612793,
      "learning_rate": 0.00018141304347826087,
      "loss": 0.9167,
      "step": 690
    },
    {
      "epoch": 1.9021739130434783,
      "grad_norm": 7.592570781707764,
      "learning_rate": 0.0001811413043478261,
      "loss": 0.9158,
      "step": 700
    },
    {
      "epoch": 1.9293478260869565,
      "grad_norm": 17.669326782226562,
      "learning_rate": 0.00018086956521739132,
      "loss": 0.8496,
      "step": 710
    },
    {
      "epoch": 1.9565217391304348,
      "grad_norm": 6.348485946655273,
      "learning_rate": 0.00018059782608695652,
      "loss": 0.816,
      "step": 720
    },
    {
      "epoch": 1.983695652173913,
      "grad_norm": 5.771228790283203,
      "learning_rate": 0.00018032608695652176,
      "loss": 0.8271,
      "step": 730
    },
    {
      "epoch": 2.010869565217391,
      "grad_norm": 7.572936058044434,
      "learning_rate": 0.00018005434782608696,
      "loss": 0.8318,
      "step": 740
    },
    {
      "epoch": 2.0380434782608696,
      "grad_norm": 4.586538791656494,
      "learning_rate": 0.00017978260869565217,
      "loss": 0.7853,
      "step": 750
    },
    {
      "epoch": 2.0652173913043477,
      "grad_norm": 6.85654354095459,
      "learning_rate": 0.0001795108695652174,
      "loss": 0.9836,
      "step": 760
    },
    {
      "epoch": 2.092391304347826,
      "grad_norm": 8.657886505126953,
      "learning_rate": 0.0001792391304347826,
      "loss": 0.6534,
      "step": 770
    },
    {
      "epoch": 2.119565217391304,
      "grad_norm": 12.145479202270508,
      "learning_rate": 0.00017896739130434782,
      "loss": 0.9362,
      "step": 780
    },
    {
      "epoch": 2.1467391304347827,
      "grad_norm": 6.326330184936523,
      "learning_rate": 0.00017869565217391305,
      "loss": 0.6039,
      "step": 790
    },
    {
      "epoch": 2.1739130434782608,
      "grad_norm": 8.368361473083496,
      "learning_rate": 0.00017842391304347826,
      "loss": 0.5178,
      "step": 800
    },
    {
      "epoch": 2.2010869565217392,
      "grad_norm": 8.297872543334961,
      "learning_rate": 0.0001781521739130435,
      "loss": 0.5989,
      "step": 810
    },
    {
      "epoch": 2.2282608695652173,
      "grad_norm": 7.059811592102051,
      "learning_rate": 0.0001778804347826087,
      "loss": 0.6752,
      "step": 820
    },
    {
      "epoch": 2.255434782608696,
      "grad_norm": 4.653237342834473,
      "learning_rate": 0.00017760869565217394,
      "loss": 0.7817,
      "step": 830
    },
    {
      "epoch": 2.282608695652174,
      "grad_norm": 4.9361724853515625,
      "learning_rate": 0.00017733695652173915,
      "loss": 0.7179,
      "step": 840
    },
    {
      "epoch": 2.3097826086956523,
      "grad_norm": 16.101863861083984,
      "learning_rate": 0.00017706521739130435,
      "loss": 0.7226,
      "step": 850
    },
    {
      "epoch": 2.3369565217391304,
      "grad_norm": 6.271755695343018,
      "learning_rate": 0.0001767934782608696,
      "loss": 0.6835,
      "step": 860
    },
    {
      "epoch": 2.364130434782609,
      "grad_norm": 7.791465759277344,
      "learning_rate": 0.0001765217391304348,
      "loss": 0.7078,
      "step": 870
    },
    {
      "epoch": 2.391304347826087,
      "grad_norm": 14.270811080932617,
      "learning_rate": 0.00017625,
      "loss": 0.8429,
      "step": 880
    },
    {
      "epoch": 2.4184782608695654,
      "grad_norm": 18.82585334777832,
      "learning_rate": 0.00017597826086956524,
      "loss": 0.6161,
      "step": 890
    },
    {
      "epoch": 2.4456521739130435,
      "grad_norm": 7.560688018798828,
      "learning_rate": 0.00017570652173913044,
      "loss": 0.8069,
      "step": 900
    },
    {
      "epoch": 2.4728260869565215,
      "grad_norm": 7.786141872406006,
      "learning_rate": 0.00017543478260869565,
      "loss": 0.8604,
      "step": 910
    },
    {
      "epoch": 2.5,
      "grad_norm": 6.147435665130615,
      "learning_rate": 0.00017516304347826086,
      "loss": 0.6326,
      "step": 920
    },
    {
      "epoch": 2.5271739130434785,
      "grad_norm": 10.907065391540527,
      "learning_rate": 0.0001748913043478261,
      "loss": 0.6984,
      "step": 930
    },
    {
      "epoch": 2.5543478260869565,
      "grad_norm": 6.045495986938477,
      "learning_rate": 0.0001746195652173913,
      "loss": 0.3669,
      "step": 940
    },
    {
      "epoch": 2.5815217391304346,
      "grad_norm": 7.072790145874023,
      "learning_rate": 0.00017434782608695653,
      "loss": 0.7715,
      "step": 950
    },
    {
      "epoch": 2.608695652173913,
      "grad_norm": 14.916816711425781,
      "learning_rate": 0.00017407608695652177,
      "loss": 0.8523,
      "step": 960
    },
    {
      "epoch": 2.6358695652173916,
      "grad_norm": 4.447398662567139,
      "learning_rate": 0.00017380434782608698,
      "loss": 0.5364,
      "step": 970
    },
    {
      "epoch": 2.6630434782608696,
      "grad_norm": 7.901666164398193,
      "learning_rate": 0.00017353260869565218,
      "loss": 0.7935,
      "step": 980
    },
    {
      "epoch": 2.6902173913043477,
      "grad_norm": 5.093670845031738,
      "learning_rate": 0.00017326086956521742,
      "loss": 0.8901,
      "step": 990
    },
    {
      "epoch": 2.717391304347826,
      "grad_norm": 3.7464680671691895,
      "learning_rate": 0.00017298913043478262,
      "loss": 0.6909,
      "step": 1000
    }
  ],
  "logging_steps": 10,
  "max_steps": 7360,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 290975144030208.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
