{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 16.304347826086957,
  "eval_steps": 500,
  "global_step": 6000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02717391304347826,
      "grad_norm": 1.858280062675476,
      "learning_rate": 0.00019978260869565217,
      "loss": 2.9263,
      "step": 10
    },
    {
      "epoch": 0.05434782608695652,
      "grad_norm": 3.812354803085327,
      "learning_rate": 0.0001995380434782609,
      "loss": 3.0221,
      "step": 20
    },
    {
      "epoch": 0.08152173913043478,
      "grad_norm": 3.969658613204956,
      "learning_rate": 0.0001992663043478261,
      "loss": 2.4169,
      "step": 30
    },
    {
      "epoch": 0.10869565217391304,
      "grad_norm": 6.3559889793396,
      "learning_rate": 0.0001989945652173913,
      "loss": 2.0587,
      "step": 40
    },
    {
      "epoch": 0.1358695652173913,
      "grad_norm": 4.14518928527832,
      "learning_rate": 0.00019880434782608696,
      "loss": 1.8471,
      "step": 50
    },
    {
      "epoch": 0.16304347826086957,
      "grad_norm": 6.662506580352783,
      "learning_rate": 0.00019853260869565217,
      "loss": 2.1131,
      "step": 60
    },
    {
      "epoch": 0.19021739130434784,
      "grad_norm": 4.655934810638428,
      "learning_rate": 0.0001982608695652174,
      "loss": 1.6704,
      "step": 70
    },
    {
      "epoch": 0.21739130434782608,
      "grad_norm": 9.094929695129395,
      "learning_rate": 0.0001979891304347826,
      "loss": 1.6821,
      "step": 80
    },
    {
      "epoch": 0.24456521739130435,
      "grad_norm": 4.5194172859191895,
      "learning_rate": 0.00019771739130434784,
      "loss": 1.5413,
      "step": 90
    },
    {
      "epoch": 0.2717391304347826,
      "grad_norm": 14.408817291259766,
      "learning_rate": 0.00019744565217391305,
      "loss": 1.574,
      "step": 100
    },
    {
      "epoch": 0.29891304347826086,
      "grad_norm": 7.112241744995117,
      "learning_rate": 0.00019717391304347828,
      "loss": 1.3409,
      "step": 110
    },
    {
      "epoch": 0.32608695652173914,
      "grad_norm": 7.8925700187683105,
      "learning_rate": 0.0001969021739130435,
      "loss": 1.5467,
      "step": 120
    },
    {
      "epoch": 0.3532608695652174,
      "grad_norm": 6.134781360626221,
      "learning_rate": 0.0001966304347826087,
      "loss": 1.6731,
      "step": 130
    },
    {
      "epoch": 0.3804347826086957,
      "grad_norm": 10.355749130249023,
      "learning_rate": 0.00019635869565217393,
      "loss": 1.2999,
      "step": 140
    },
    {
      "epoch": 0.4076086956521739,
      "grad_norm": 4.096418380737305,
      "learning_rate": 0.00019608695652173914,
      "loss": 1.5674,
      "step": 150
    },
    {
      "epoch": 0.43478260869565216,
      "grad_norm": 4.768747806549072,
      "learning_rate": 0.00019581521739130435,
      "loss": 1.1745,
      "step": 160
    },
    {
      "epoch": 0.46195652173913043,
      "grad_norm": 6.453281879425049,
      "learning_rate": 0.00019554347826086958,
      "loss": 1.3976,
      "step": 170
    },
    {
      "epoch": 0.4891304347826087,
      "grad_norm": 4.45206356048584,
      "learning_rate": 0.0001952717391304348,
      "loss": 1.5724,
      "step": 180
    },
    {
      "epoch": 0.5163043478260869,
      "grad_norm": 5.2901411056518555,
      "learning_rate": 0.000195,
      "loss": 1.5382,
      "step": 190
    },
    {
      "epoch": 0.5434782608695652,
      "grad_norm": 4.825442314147949,
      "learning_rate": 0.00019472826086956523,
      "loss": 1.4633,
      "step": 200
    },
    {
      "epoch": 0.5706521739130435,
      "grad_norm": 6.178434371948242,
      "learning_rate": 0.00019445652173913044,
      "loss": 1.4216,
      "step": 210
    },
    {
      "epoch": 0.5978260869565217,
      "grad_norm": 7.09343957901001,
      "learning_rate": 0.00019418478260869567,
      "loss": 1.4035,
      "step": 220
    },
    {
      "epoch": 0.625,
      "grad_norm": 7.159855842590332,
      "learning_rate": 0.00019391304347826088,
      "loss": 1.4828,
      "step": 230
    },
    {
      "epoch": 0.6521739130434783,
      "grad_norm": 5.902719974517822,
      "learning_rate": 0.00019364130434782611,
      "loss": 1.2125,
      "step": 240
    },
    {
      "epoch": 0.6793478260869565,
      "grad_norm": 4.84725284576416,
      "learning_rate": 0.00019336956521739132,
      "loss": 1.4477,
      "step": 250
    },
    {
      "epoch": 0.7065217391304348,
      "grad_norm": 4.909605026245117,
      "learning_rate": 0.00019309782608695653,
      "loss": 1.217,
      "step": 260
    },
    {
      "epoch": 0.7336956521739131,
      "grad_norm": 6.655997276306152,
      "learning_rate": 0.00019282608695652176,
      "loss": 1.297,
      "step": 270
    },
    {
      "epoch": 0.7608695652173914,
      "grad_norm": 4.5640764236450195,
      "learning_rate": 0.00019255434782608697,
      "loss": 1.4114,
      "step": 280
    },
    {
      "epoch": 0.7880434782608695,
      "grad_norm": 4.893315315246582,
      "learning_rate": 0.00019228260869565218,
      "loss": 1.086,
      "step": 290
    },
    {
      "epoch": 0.8152173913043478,
      "grad_norm": 9.92303466796875,
      "learning_rate": 0.0001920108695652174,
      "loss": 1.4338,
      "step": 300
    },
    {
      "epoch": 0.842391304347826,
      "grad_norm": 7.21193265914917,
      "learning_rate": 0.00019173913043478262,
      "loss": 1.2129,
      "step": 310
    },
    {
      "epoch": 0.8695652173913043,
      "grad_norm": 7.024837970733643,
      "learning_rate": 0.00019146739130434783,
      "loss": 1.2183,
      "step": 320
    },
    {
      "epoch": 0.8967391304347826,
      "grad_norm": 6.16318941116333,
      "learning_rate": 0.00019119565217391303,
      "loss": 1.0701,
      "step": 330
    },
    {
      "epoch": 0.9239130434782609,
      "grad_norm": 6.199466228485107,
      "learning_rate": 0.00019092391304347827,
      "loss": 1.2121,
      "step": 340
    },
    {
      "epoch": 0.9510869565217391,
      "grad_norm": 5.623912811279297,
      "learning_rate": 0.00019065217391304348,
      "loss": 1.3395,
      "step": 350
    },
    {
      "epoch": 0.9782608695652174,
      "grad_norm": 6.265106201171875,
      "learning_rate": 0.0001903804347826087,
      "loss": 1.2074,
      "step": 360
    },
    {
      "epoch": 1.0054347826086956,
      "grad_norm": 7.836062431335449,
      "learning_rate": 0.00019010869565217392,
      "loss": 1.1569,
      "step": 370
    },
    {
      "epoch": 1.0326086956521738,
      "grad_norm": 6.503505229949951,
      "learning_rate": 0.00018983695652173915,
      "loss": 0.9372,
      "step": 380
    },
    {
      "epoch": 1.059782608695652,
      "grad_norm": 6.2892985343933105,
      "learning_rate": 0.00018956521739130436,
      "loss": 1.1895,
      "step": 390
    },
    {
      "epoch": 1.0869565217391304,
      "grad_norm": 7.21104097366333,
      "learning_rate": 0.0001892934782608696,
      "loss": 1.4854,
      "step": 400
    },
    {
      "epoch": 1.1141304347826086,
      "grad_norm": 7.502607345581055,
      "learning_rate": 0.0001890217391304348,
      "loss": 1.0431,
      "step": 410
    },
    {
      "epoch": 1.141304347826087,
      "grad_norm": 5.04808235168457,
      "learning_rate": 0.00018875,
      "loss": 1.1058,
      "step": 420
    },
    {
      "epoch": 1.1684782608695652,
      "grad_norm": 4.976306915283203,
      "learning_rate": 0.00018847826086956524,
      "loss": 0.9103,
      "step": 430
    },
    {
      "epoch": 1.1956521739130435,
      "grad_norm": 8.628564834594727,
      "learning_rate": 0.00018820652173913045,
      "loss": 1.1287,
      "step": 440
    },
    {
      "epoch": 1.2228260869565217,
      "grad_norm": 16.884870529174805,
      "learning_rate": 0.00018793478260869566,
      "loss": 0.9912,
      "step": 450
    },
    {
      "epoch": 1.25,
      "grad_norm": 5.307351589202881,
      "learning_rate": 0.00018766304347826086,
      "loss": 0.9098,
      "step": 460
    },
    {
      "epoch": 1.2771739130434783,
      "grad_norm": 8.836557388305664,
      "learning_rate": 0.0001873913043478261,
      "loss": 0.8656,
      "step": 470
    },
    {
      "epoch": 1.3043478260869565,
      "grad_norm": 8.308188438415527,
      "learning_rate": 0.0001871195652173913,
      "loss": 1.3327,
      "step": 480
    },
    {
      "epoch": 1.3315217391304348,
      "grad_norm": 6.589860916137695,
      "learning_rate": 0.0001868478260869565,
      "loss": 0.8747,
      "step": 490
    },
    {
      "epoch": 1.358695652173913,
      "grad_norm": 3.953298807144165,
      "learning_rate": 0.00018657608695652175,
      "loss": 0.8456,
      "step": 500
    },
    {
      "epoch": 1.3858695652173914,
      "grad_norm": 11.325197219848633,
      "learning_rate": 0.00018630434782608695,
      "loss": 0.9837,
      "step": 510
    },
    {
      "epoch": 1.4130434782608696,
      "grad_norm": 7.573903560638428,
      "learning_rate": 0.0001860326086956522,
      "loss": 1.257,
      "step": 520
    },
    {
      "epoch": 1.440217391304348,
      "grad_norm": 6.222319602966309,
      "learning_rate": 0.00018576086956521742,
      "loss": 0.8892,
      "step": 530
    },
    {
      "epoch": 1.4673913043478262,
      "grad_norm": 11.877246856689453,
      "learning_rate": 0.00018548913043478263,
      "loss": 1.2208,
      "step": 540
    },
    {
      "epoch": 1.4945652173913042,
      "grad_norm": 4.6044158935546875,
      "learning_rate": 0.00018521739130434784,
      "loss": 1.1729,
      "step": 550
    },
    {
      "epoch": 1.5217391304347827,
      "grad_norm": 9.579429626464844,
      "learning_rate": 0.00018494565217391304,
      "loss": 1.1511,
      "step": 560
    },
    {
      "epoch": 1.5489130434782608,
      "grad_norm": 11.49062442779541,
      "learning_rate": 0.00018467391304347828,
      "loss": 0.9277,
      "step": 570
    },
    {
      "epoch": 1.5760869565217392,
      "grad_norm": 7.303023815155029,
      "learning_rate": 0.00018440217391304349,
      "loss": 0.9939,
      "step": 580
    },
    {
      "epoch": 1.6032608695652173,
      "grad_norm": 9.111318588256836,
      "learning_rate": 0.0001841304347826087,
      "loss": 1.0109,
      "step": 590
    },
    {
      "epoch": 1.6304347826086958,
      "grad_norm": 8.42862606048584,
      "learning_rate": 0.00018385869565217393,
      "loss": 0.8483,
      "step": 600
    },
    {
      "epoch": 1.6576086956521738,
      "grad_norm": 4.577413558959961,
      "learning_rate": 0.00018358695652173913,
      "loss": 1.0883,
      "step": 610
    },
    {
      "epoch": 1.6847826086956523,
      "grad_norm": 5.787291526794434,
      "learning_rate": 0.00018331521739130434,
      "loss": 0.7834,
      "step": 620
    },
    {
      "epoch": 1.7119565217391304,
      "grad_norm": 6.509949207305908,
      "learning_rate": 0.00018304347826086958,
      "loss": 0.9003,
      "step": 630
    },
    {
      "epoch": 1.7391304347826086,
      "grad_norm": 4.616988182067871,
      "learning_rate": 0.00018277173913043478,
      "loss": 1.0329,
      "step": 640
    },
    {
      "epoch": 1.766304347826087,
      "grad_norm": 9.458673477172852,
      "learning_rate": 0.0001825,
      "loss": 0.8337,
      "step": 650
    },
    {
      "epoch": 1.7934782608695652,
      "grad_norm": 8.935589790344238,
      "learning_rate": 0.00018222826086956523,
      "loss": 0.707,
      "step": 660
    },
    {
      "epoch": 1.8206521739130435,
      "grad_norm": 7.959635257720947,
      "learning_rate": 0.00018195652173913046,
      "loss": 1.1736,
      "step": 670
    },
    {
      "epoch": 1.8478260869565217,
      "grad_norm": 8.486064910888672,
      "learning_rate": 0.00018168478260869567,
      "loss": 1.3442,
      "step": 680
    },
    {
      "epoch": 1.875,
      "grad_norm": 5.481043815612793,
      "learning_rate": 0.00018141304347826087,
      "loss": 0.9167,
      "step": 690
    },
    {
      "epoch": 1.9021739130434783,
      "grad_norm": 7.592570781707764,
      "learning_rate": 0.0001811413043478261,
      "loss": 0.9158,
      "step": 700
    },
    {
      "epoch": 1.9293478260869565,
      "grad_norm": 17.669326782226562,
      "learning_rate": 0.00018086956521739132,
      "loss": 0.8496,
      "step": 710
    },
    {
      "epoch": 1.9565217391304348,
      "grad_norm": 6.348485946655273,
      "learning_rate": 0.00018059782608695652,
      "loss": 0.816,
      "step": 720
    },
    {
      "epoch": 1.983695652173913,
      "grad_norm": 5.771228790283203,
      "learning_rate": 0.00018032608695652176,
      "loss": 0.8271,
      "step": 730
    },
    {
      "epoch": 2.010869565217391,
      "grad_norm": 7.572936058044434,
      "learning_rate": 0.00018005434782608696,
      "loss": 0.8318,
      "step": 740
    },
    {
      "epoch": 2.0380434782608696,
      "grad_norm": 4.586538791656494,
      "learning_rate": 0.00017978260869565217,
      "loss": 0.7853,
      "step": 750
    },
    {
      "epoch": 2.0652173913043477,
      "grad_norm": 6.85654354095459,
      "learning_rate": 0.0001795108695652174,
      "loss": 0.9836,
      "step": 760
    },
    {
      "epoch": 2.092391304347826,
      "grad_norm": 8.657886505126953,
      "learning_rate": 0.0001792391304347826,
      "loss": 0.6534,
      "step": 770
    },
    {
      "epoch": 2.119565217391304,
      "grad_norm": 12.145479202270508,
      "learning_rate": 0.00017896739130434782,
      "loss": 0.9362,
      "step": 780
    },
    {
      "epoch": 2.1467391304347827,
      "grad_norm": 6.326330184936523,
      "learning_rate": 0.00017869565217391305,
      "loss": 0.6039,
      "step": 790
    },
    {
      "epoch": 2.1739130434782608,
      "grad_norm": 8.368361473083496,
      "learning_rate": 0.00017842391304347826,
      "loss": 0.5178,
      "step": 800
    },
    {
      "epoch": 2.2010869565217392,
      "grad_norm": 8.297872543334961,
      "learning_rate": 0.0001781521739130435,
      "loss": 0.5989,
      "step": 810
    },
    {
      "epoch": 2.2282608695652173,
      "grad_norm": 7.059811592102051,
      "learning_rate": 0.0001778804347826087,
      "loss": 0.6752,
      "step": 820
    },
    {
      "epoch": 2.255434782608696,
      "grad_norm": 4.653237342834473,
      "learning_rate": 0.00017760869565217394,
      "loss": 0.7817,
      "step": 830
    },
    {
      "epoch": 2.282608695652174,
      "grad_norm": 4.9361724853515625,
      "learning_rate": 0.00017733695652173915,
      "loss": 0.7179,
      "step": 840
    },
    {
      "epoch": 2.3097826086956523,
      "grad_norm": 16.101863861083984,
      "learning_rate": 0.00017706521739130435,
      "loss": 0.7226,
      "step": 850
    },
    {
      "epoch": 2.3369565217391304,
      "grad_norm": 6.271755695343018,
      "learning_rate": 0.0001767934782608696,
      "loss": 0.6835,
      "step": 860
    },
    {
      "epoch": 2.364130434782609,
      "grad_norm": 7.791465759277344,
      "learning_rate": 0.0001765217391304348,
      "loss": 0.7078,
      "step": 870
    },
    {
      "epoch": 2.391304347826087,
      "grad_norm": 14.270811080932617,
      "learning_rate": 0.00017625,
      "loss": 0.8429,
      "step": 880
    },
    {
      "epoch": 2.4184782608695654,
      "grad_norm": 18.82585334777832,
      "learning_rate": 0.00017597826086956524,
      "loss": 0.6161,
      "step": 890
    },
    {
      "epoch": 2.4456521739130435,
      "grad_norm": 7.560688018798828,
      "learning_rate": 0.00017570652173913044,
      "loss": 0.8069,
      "step": 900
    },
    {
      "epoch": 2.4728260869565215,
      "grad_norm": 7.786141872406006,
      "learning_rate": 0.00017543478260869565,
      "loss": 0.8604,
      "step": 910
    },
    {
      "epoch": 2.5,
      "grad_norm": 6.147435665130615,
      "learning_rate": 0.00017516304347826086,
      "loss": 0.6326,
      "step": 920
    },
    {
      "epoch": 2.5271739130434785,
      "grad_norm": 10.907065391540527,
      "learning_rate": 0.0001748913043478261,
      "loss": 0.6984,
      "step": 930
    },
    {
      "epoch": 2.5543478260869565,
      "grad_norm": 6.045495986938477,
      "learning_rate": 0.0001746195652173913,
      "loss": 0.3669,
      "step": 940
    },
    {
      "epoch": 2.5815217391304346,
      "grad_norm": 7.072790145874023,
      "learning_rate": 0.00017434782608695653,
      "loss": 0.7715,
      "step": 950
    },
    {
      "epoch": 2.608695652173913,
      "grad_norm": 14.916816711425781,
      "learning_rate": 0.00017407608695652177,
      "loss": 0.8523,
      "step": 960
    },
    {
      "epoch": 2.6358695652173916,
      "grad_norm": 4.447398662567139,
      "learning_rate": 0.00017380434782608698,
      "loss": 0.5364,
      "step": 970
    },
    {
      "epoch": 2.6630434782608696,
      "grad_norm": 7.901666164398193,
      "learning_rate": 0.00017353260869565218,
      "loss": 0.7935,
      "step": 980
    },
    {
      "epoch": 2.6902173913043477,
      "grad_norm": 5.093670845031738,
      "learning_rate": 0.00017326086956521742,
      "loss": 0.8901,
      "step": 990
    },
    {
      "epoch": 2.717391304347826,
      "grad_norm": 3.7464680671691895,
      "learning_rate": 0.00017298913043478262,
      "loss": 0.6909,
      "step": 1000
    },
    {
      "epoch": 2.744565217391304,
      "grad_norm": 23.273929595947266,
      "learning_rate": 0.00017271739130434783,
      "loss": 0.5745,
      "step": 1010
    },
    {
      "epoch": 2.7717391304347827,
      "grad_norm": 8.754461288452148,
      "learning_rate": 0.00017244565217391304,
      "loss": 0.7285,
      "step": 1020
    },
    {
      "epoch": 2.7989130434782608,
      "grad_norm": 13.669981002807617,
      "learning_rate": 0.00017217391304347827,
      "loss": 0.8681,
      "step": 1030
    },
    {
      "epoch": 2.8260869565217392,
      "grad_norm": 6.118854999542236,
      "learning_rate": 0.00017190217391304348,
      "loss": 0.7281,
      "step": 1040
    },
    {
      "epoch": 2.8532608695652173,
      "grad_norm": 4.475048065185547,
      "learning_rate": 0.0001716304347826087,
      "loss": 0.8667,
      "step": 1050
    },
    {
      "epoch": 2.880434782608696,
      "grad_norm": 5.623793601989746,
      "learning_rate": 0.00017135869565217392,
      "loss": 0.8481,
      "step": 1060
    },
    {
      "epoch": 2.907608695652174,
      "grad_norm": 7.792527198791504,
      "learning_rate": 0.00017108695652173913,
      "loss": 0.5169,
      "step": 1070
    },
    {
      "epoch": 2.9347826086956523,
      "grad_norm": 5.2684760093688965,
      "learning_rate": 0.00017081521739130434,
      "loss": 0.5882,
      "step": 1080
    },
    {
      "epoch": 2.9619565217391304,
      "grad_norm": 7.441559314727783,
      "learning_rate": 0.00017054347826086957,
      "loss": 0.6295,
      "step": 1090
    },
    {
      "epoch": 2.9891304347826084,
      "grad_norm": 6.77869176864624,
      "learning_rate": 0.0001702717391304348,
      "loss": 0.7626,
      "step": 1100
    },
    {
      "epoch": 3.016304347826087,
      "grad_norm": 8.908666610717773,
      "learning_rate": 0.00017,
      "loss": 0.8034,
      "step": 1110
    },
    {
      "epoch": 3.0434782608695654,
      "grad_norm": 8.713645935058594,
      "learning_rate": 0.00016972826086956525,
      "loss": 0.5125,
      "step": 1120
    },
    {
      "epoch": 3.0706521739130435,
      "grad_norm": 8.309712409973145,
      "learning_rate": 0.00016945652173913045,
      "loss": 0.3132,
      "step": 1130
    },
    {
      "epoch": 3.097826086956522,
      "grad_norm": 3.2395331859588623,
      "learning_rate": 0.00016918478260869566,
      "loss": 0.6469,
      "step": 1140
    },
    {
      "epoch": 3.125,
      "grad_norm": 8.101088523864746,
      "learning_rate": 0.00016891304347826087,
      "loss": 0.5483,
      "step": 1150
    },
    {
      "epoch": 3.1521739130434785,
      "grad_norm": 7.493526935577393,
      "learning_rate": 0.0001686413043478261,
      "loss": 0.3987,
      "step": 1160
    },
    {
      "epoch": 3.1793478260869565,
      "grad_norm": 2.6663830280303955,
      "learning_rate": 0.0001683695652173913,
      "loss": 0.607,
      "step": 1170
    },
    {
      "epoch": 3.2065217391304346,
      "grad_norm": 15.951591491699219,
      "learning_rate": 0.00016809782608695652,
      "loss": 0.6993,
      "step": 1180
    },
    {
      "epoch": 3.233695652173913,
      "grad_norm": 5.776637554168701,
      "learning_rate": 0.00016782608695652175,
      "loss": 0.4661,
      "step": 1190
    },
    {
      "epoch": 3.260869565217391,
      "grad_norm": 8.072189331054688,
      "learning_rate": 0.00016755434782608696,
      "loss": 0.8024,
      "step": 1200
    },
    {
      "epoch": 3.2880434782608696,
      "grad_norm": 8.270376205444336,
      "learning_rate": 0.00016728260869565217,
      "loss": 0.7339,
      "step": 1210
    },
    {
      "epoch": 3.3152173913043477,
      "grad_norm": 4.4508256912231445,
      "learning_rate": 0.0001670108695652174,
      "loss": 0.5315,
      "step": 1220
    },
    {
      "epoch": 3.342391304347826,
      "grad_norm": 3.155388832092285,
      "learning_rate": 0.0001667391304347826,
      "loss": 0.4134,
      "step": 1230
    },
    {
      "epoch": 3.369565217391304,
      "grad_norm": 4.119101524353027,
      "learning_rate": 0.00016646739130434784,
      "loss": 0.6679,
      "step": 1240
    },
    {
      "epoch": 3.3967391304347827,
      "grad_norm": 8.141707420349121,
      "learning_rate": 0.00016619565217391305,
      "loss": 0.6586,
      "step": 1250
    },
    {
      "epoch": 3.4239130434782608,
      "grad_norm": 6.491395473480225,
      "learning_rate": 0.00016592391304347828,
      "loss": 0.5019,
      "step": 1260
    },
    {
      "epoch": 3.4510869565217392,
      "grad_norm": 6.105682849884033,
      "learning_rate": 0.0001656521739130435,
      "loss": 0.5691,
      "step": 1270
    },
    {
      "epoch": 3.4782608695652173,
      "grad_norm": 3.756046772003174,
      "learning_rate": 0.0001653804347826087,
      "loss": 0.7789,
      "step": 1280
    },
    {
      "epoch": 3.505434782608696,
      "grad_norm": 9.641332626342773,
      "learning_rate": 0.00016510869565217393,
      "loss": 0.3674,
      "step": 1290
    },
    {
      "epoch": 3.532608695652174,
      "grad_norm": 2.6092309951782227,
      "learning_rate": 0.00016483695652173914,
      "loss": 0.585,
      "step": 1300
    },
    {
      "epoch": 3.5597826086956523,
      "grad_norm": 7.8287672996521,
      "learning_rate": 0.00016456521739130435,
      "loss": 0.5125,
      "step": 1310
    },
    {
      "epoch": 3.5869565217391304,
      "grad_norm": 16.787534713745117,
      "learning_rate": 0.00016429347826086958,
      "loss": 0.4545,
      "step": 1320
    },
    {
      "epoch": 3.6141304347826084,
      "grad_norm": 3.067957878112793,
      "learning_rate": 0.0001640217391304348,
      "loss": 0.5478,
      "step": 1330
    },
    {
      "epoch": 3.641304347826087,
      "grad_norm": 12.857972145080566,
      "learning_rate": 0.00016375,
      "loss": 0.5189,
      "step": 1340
    },
    {
      "epoch": 3.6684782608695654,
      "grad_norm": 3.7038424015045166,
      "learning_rate": 0.00016347826086956523,
      "loss": 0.6788,
      "step": 1350
    },
    {
      "epoch": 3.6956521739130435,
      "grad_norm": 8.41106128692627,
      "learning_rate": 0.00016320652173913044,
      "loss": 0.3989,
      "step": 1360
    },
    {
      "epoch": 3.7228260869565215,
      "grad_norm": 10.420360565185547,
      "learning_rate": 0.00016293478260869564,
      "loss": 0.4126,
      "step": 1370
    },
    {
      "epoch": 3.75,
      "grad_norm": 3.7935290336608887,
      "learning_rate": 0.00016266304347826088,
      "loss": 0.4558,
      "step": 1380
    },
    {
      "epoch": 3.7771739130434785,
      "grad_norm": 6.457462310791016,
      "learning_rate": 0.0001623913043478261,
      "loss": 0.5445,
      "step": 1390
    },
    {
      "epoch": 3.8043478260869565,
      "grad_norm": 6.4192728996276855,
      "learning_rate": 0.00016211956521739132,
      "loss": 0.6477,
      "step": 1400
    },
    {
      "epoch": 3.8315217391304346,
      "grad_norm": 5.814953804016113,
      "learning_rate": 0.00016184782608695653,
      "loss": 0.5667,
      "step": 1410
    },
    {
      "epoch": 3.858695652173913,
      "grad_norm": 11.639525413513184,
      "learning_rate": 0.00016157608695652176,
      "loss": 0.5157,
      "step": 1420
    },
    {
      "epoch": 3.8858695652173916,
      "grad_norm": 14.786423683166504,
      "learning_rate": 0.00016130434782608697,
      "loss": 0.385,
      "step": 1430
    },
    {
      "epoch": 3.9130434782608696,
      "grad_norm": 3.081246852874756,
      "learning_rate": 0.00016103260869565218,
      "loss": 0.5359,
      "step": 1440
    },
    {
      "epoch": 3.9402173913043477,
      "grad_norm": 4.925380229949951,
      "learning_rate": 0.0001607608695652174,
      "loss": 0.4709,
      "step": 1450
    },
    {
      "epoch": 3.967391304347826,
      "grad_norm": 4.958588123321533,
      "learning_rate": 0.00016048913043478262,
      "loss": 1.0139,
      "step": 1460
    },
    {
      "epoch": 3.994565217391304,
      "grad_norm": 4.055169582366943,
      "learning_rate": 0.00016021739130434783,
      "loss": 0.5163,
      "step": 1470
    },
    {
      "epoch": 4.021739130434782,
      "grad_norm": 4.243257999420166,
      "learning_rate": 0.00015994565217391303,
      "loss": 0.3742,
      "step": 1480
    },
    {
      "epoch": 4.048913043478261,
      "grad_norm": 2.7159829139709473,
      "learning_rate": 0.00015967391304347827,
      "loss": 0.2581,
      "step": 1490
    },
    {
      "epoch": 4.076086956521739,
      "grad_norm": 16.108142852783203,
      "learning_rate": 0.00015940217391304347,
      "loss": 0.4461,
      "step": 1500
    },
    {
      "epoch": 4.103260869565218,
      "grad_norm": 5.264904975891113,
      "learning_rate": 0.00015913043478260868,
      "loss": 0.2546,
      "step": 1510
    },
    {
      "epoch": 4.130434782608695,
      "grad_norm": 9.624746322631836,
      "learning_rate": 0.00015885869565217392,
      "loss": 0.4802,
      "step": 1520
    },
    {
      "epoch": 4.157608695652174,
      "grad_norm": 15.584049224853516,
      "learning_rate": 0.00015858695652173915,
      "loss": 0.4734,
      "step": 1530
    },
    {
      "epoch": 4.184782608695652,
      "grad_norm": 8.043275833129883,
      "learning_rate": 0.00015831521739130436,
      "loss": 0.4166,
      "step": 1540
    },
    {
      "epoch": 4.211956521739131,
      "grad_norm": 11.536609649658203,
      "learning_rate": 0.0001580434782608696,
      "loss": 0.3985,
      "step": 1550
    },
    {
      "epoch": 4.239130434782608,
      "grad_norm": 6.786740303039551,
      "learning_rate": 0.0001577717391304348,
      "loss": 0.4805,
      "step": 1560
    },
    {
      "epoch": 4.266304347826087,
      "grad_norm": 3.2535479068756104,
      "learning_rate": 0.0001575,
      "loss": 0.7133,
      "step": 1570
    },
    {
      "epoch": 4.293478260869565,
      "grad_norm": 15.917353630065918,
      "learning_rate": 0.00015722826086956524,
      "loss": 0.4407,
      "step": 1580
    },
    {
      "epoch": 4.320652173913043,
      "grad_norm": 3.347257375717163,
      "learning_rate": 0.00015695652173913045,
      "loss": 0.3788,
      "step": 1590
    },
    {
      "epoch": 4.3478260869565215,
      "grad_norm": 15.31794548034668,
      "learning_rate": 0.00015668478260869566,
      "loss": 0.4933,
      "step": 1600
    },
    {
      "epoch": 4.375,
      "grad_norm": 2.679299831390381,
      "learning_rate": 0.00015641304347826086,
      "loss": 0.7533,
      "step": 1610
    },
    {
      "epoch": 4.4021739130434785,
      "grad_norm": 2.1892080307006836,
      "learning_rate": 0.0001561413043478261,
      "loss": 0.3699,
      "step": 1620
    },
    {
      "epoch": 4.429347826086957,
      "grad_norm": 2.8088433742523193,
      "learning_rate": 0.0001558695652173913,
      "loss": 0.5604,
      "step": 1630
    },
    {
      "epoch": 4.456521739130435,
      "grad_norm": 10.85625171661377,
      "learning_rate": 0.0001555978260869565,
      "loss": 0.6355,
      "step": 1640
    },
    {
      "epoch": 4.483695652173913,
      "grad_norm": 9.10905933380127,
      "learning_rate": 0.00015532608695652175,
      "loss": 0.3576,
      "step": 1650
    },
    {
      "epoch": 4.510869565217392,
      "grad_norm": 3.6806771755218506,
      "learning_rate": 0.00015505434782608695,
      "loss": 0.4254,
      "step": 1660
    },
    {
      "epoch": 4.538043478260869,
      "grad_norm": 2.325397491455078,
      "learning_rate": 0.0001547826086956522,
      "loss": 0.2911,
      "step": 1670
    },
    {
      "epoch": 4.565217391304348,
      "grad_norm": 7.603055477142334,
      "learning_rate": 0.00015451086956521742,
      "loss": 0.5503,
      "step": 1680
    },
    {
      "epoch": 4.592391304347826,
      "grad_norm": 7.375704765319824,
      "learning_rate": 0.00015423913043478263,
      "loss": 0.4288,
      "step": 1690
    },
    {
      "epoch": 4.619565217391305,
      "grad_norm": 1.5209972858428955,
      "learning_rate": 0.00015396739130434784,
      "loss": 0.4747,
      "step": 1700
    },
    {
      "epoch": 4.646739130434782,
      "grad_norm": 9.623878479003906,
      "learning_rate": 0.00015369565217391304,
      "loss": 0.4402,
      "step": 1710
    },
    {
      "epoch": 4.673913043478261,
      "grad_norm": 7.4816789627075195,
      "learning_rate": 0.00015342391304347828,
      "loss": 0.5101,
      "step": 1720
    },
    {
      "epoch": 4.701086956521739,
      "grad_norm": 3.9208226203918457,
      "learning_rate": 0.00015315217391304349,
      "loss": 0.509,
      "step": 1730
    },
    {
      "epoch": 4.728260869565218,
      "grad_norm": 8.292793273925781,
      "learning_rate": 0.0001528804347826087,
      "loss": 0.5125,
      "step": 1740
    },
    {
      "epoch": 4.755434782608695,
      "grad_norm": 7.396771430969238,
      "learning_rate": 0.00015260869565217393,
      "loss": 0.5539,
      "step": 1750
    },
    {
      "epoch": 4.782608695652174,
      "grad_norm": 3.2250308990478516,
      "learning_rate": 0.00015233695652173913,
      "loss": 0.2917,
      "step": 1760
    },
    {
      "epoch": 4.809782608695652,
      "grad_norm": 2.323725700378418,
      "learning_rate": 0.00015206521739130434,
      "loss": 0.4349,
      "step": 1770
    },
    {
      "epoch": 4.836956521739131,
      "grad_norm": 1.8828729391098022,
      "learning_rate": 0.00015179347826086958,
      "loss": 0.3919,
      "step": 1780
    },
    {
      "epoch": 4.864130434782608,
      "grad_norm": 10.757002830505371,
      "learning_rate": 0.00015152173913043478,
      "loss": 0.5495,
      "step": 1790
    },
    {
      "epoch": 4.891304347826087,
      "grad_norm": 1.994342565536499,
      "learning_rate": 0.00015125,
      "loss": 0.6109,
      "step": 1800
    },
    {
      "epoch": 4.918478260869565,
      "grad_norm": 3.2052273750305176,
      "learning_rate": 0.00015097826086956522,
      "loss": 0.4837,
      "step": 1810
    },
    {
      "epoch": 4.945652173913043,
      "grad_norm": 1.6927425861358643,
      "learning_rate": 0.00015070652173913046,
      "loss": 0.3306,
      "step": 1820
    },
    {
      "epoch": 4.9728260869565215,
      "grad_norm": 6.0148749351501465,
      "learning_rate": 0.00015043478260869567,
      "loss": 0.6442,
      "step": 1830
    },
    {
      "epoch": 5.0,
      "grad_norm": 8.525575637817383,
      "learning_rate": 0.00015016304347826087,
      "loss": 0.5555,
      "step": 1840
    },
    {
      "epoch": 5.0271739130434785,
      "grad_norm": 13.287069320678711,
      "learning_rate": 0.0001498913043478261,
      "loss": 0.4998,
      "step": 1850
    },
    {
      "epoch": 5.054347826086956,
      "grad_norm": 4.976263523101807,
      "learning_rate": 0.00014961956521739131,
      "loss": 0.352,
      "step": 1860
    },
    {
      "epoch": 5.081521739130435,
      "grad_norm": 5.484924793243408,
      "learning_rate": 0.00014934782608695652,
      "loss": 0.3168,
      "step": 1870
    },
    {
      "epoch": 5.108695652173913,
      "grad_norm": 2.1795637607574463,
      "learning_rate": 0.00014907608695652176,
      "loss": 0.3432,
      "step": 1880
    },
    {
      "epoch": 5.135869565217392,
      "grad_norm": 5.286979675292969,
      "learning_rate": 0.00014880434782608696,
      "loss": 0.4772,
      "step": 1890
    },
    {
      "epoch": 5.163043478260869,
      "grad_norm": 9.869776725769043,
      "learning_rate": 0.00014853260869565217,
      "loss": 0.3634,
      "step": 1900
    },
    {
      "epoch": 5.190217391304348,
      "grad_norm": 7.224443435668945,
      "learning_rate": 0.0001482608695652174,
      "loss": 0.2226,
      "step": 1910
    },
    {
      "epoch": 5.217391304347826,
      "grad_norm": 5.610621929168701,
      "learning_rate": 0.0001479891304347826,
      "loss": 0.4266,
      "step": 1920
    },
    {
      "epoch": 5.244565217391305,
      "grad_norm": 1.4266855716705322,
      "learning_rate": 0.00014771739130434782,
      "loss": 0.4462,
      "step": 1930
    },
    {
      "epoch": 5.271739130434782,
      "grad_norm": 10.148588180541992,
      "learning_rate": 0.00014744565217391303,
      "loss": 0.5335,
      "step": 1940
    },
    {
      "epoch": 5.298913043478261,
      "grad_norm": 6.675113677978516,
      "learning_rate": 0.00014717391304347826,
      "loss": 0.4239,
      "step": 1950
    },
    {
      "epoch": 5.326086956521739,
      "grad_norm": 5.7545166015625,
      "learning_rate": 0.0001469021739130435,
      "loss": 0.2481,
      "step": 1960
    },
    {
      "epoch": 5.353260869565218,
      "grad_norm": 1.5352249145507812,
      "learning_rate": 0.0001466304347826087,
      "loss": 0.324,
      "step": 1970
    },
    {
      "epoch": 5.380434782608695,
      "grad_norm": 2.4577279090881348,
      "learning_rate": 0.00014635869565217394,
      "loss": 0.3741,
      "step": 1980
    },
    {
      "epoch": 5.407608695652174,
      "grad_norm": 5.735659599304199,
      "learning_rate": 0.00014608695652173914,
      "loss": 0.3439,
      "step": 1990
    },
    {
      "epoch": 5.434782608695652,
      "grad_norm": 11.22774600982666,
      "learning_rate": 0.00014581521739130435,
      "loss": 0.286,
      "step": 2000
    },
    {
      "epoch": 5.461956521739131,
      "grad_norm": 7.880152702331543,
      "learning_rate": 0.00014554347826086959,
      "loss": 0.4525,
      "step": 2010
    },
    {
      "epoch": 5.489130434782608,
      "grad_norm": 5.063705921173096,
      "learning_rate": 0.0001452717391304348,
      "loss": 0.3794,
      "step": 2020
    },
    {
      "epoch": 5.516304347826087,
      "grad_norm": 3.987565040588379,
      "learning_rate": 0.000145,
      "loss": 0.4665,
      "step": 2030
    },
    {
      "epoch": 5.543478260869565,
      "grad_norm": 4.992577075958252,
      "learning_rate": 0.00014472826086956524,
      "loss": 0.3204,
      "step": 2040
    },
    {
      "epoch": 5.570652173913043,
      "grad_norm": 2.827362060546875,
      "learning_rate": 0.00014445652173913044,
      "loss": 0.2872,
      "step": 2050
    },
    {
      "epoch": 5.5978260869565215,
      "grad_norm": 3.7392470836639404,
      "learning_rate": 0.00014418478260869565,
      "loss": 0.1816,
      "step": 2060
    },
    {
      "epoch": 5.625,
      "grad_norm": 2.9073963165283203,
      "learning_rate": 0.00014391304347826086,
      "loss": 0.4184,
      "step": 2070
    },
    {
      "epoch": 5.6521739130434785,
      "grad_norm": 2.7068607807159424,
      "learning_rate": 0.0001436413043478261,
      "loss": 0.2623,
      "step": 2080
    },
    {
      "epoch": 5.679347826086957,
      "grad_norm": 2.198199987411499,
      "learning_rate": 0.0001433695652173913,
      "loss": 0.3873,
      "step": 2090
    },
    {
      "epoch": 5.706521739130435,
      "grad_norm": 2.8936281204223633,
      "learning_rate": 0.00014309782608695653,
      "loss": 0.4653,
      "step": 2100
    },
    {
      "epoch": 5.733695652173913,
      "grad_norm": 9.316268920898438,
      "learning_rate": 0.00014282608695652177,
      "loss": 0.569,
      "step": 2110
    },
    {
      "epoch": 5.760869565217392,
      "grad_norm": 6.9274702072143555,
      "learning_rate": 0.00014255434782608697,
      "loss": 0.6278,
      "step": 2120
    },
    {
      "epoch": 5.788043478260869,
      "grad_norm": 11.430323600769043,
      "learning_rate": 0.00014228260869565218,
      "loss": 0.6503,
      "step": 2130
    },
    {
      "epoch": 5.815217391304348,
      "grad_norm": 7.51364278793335,
      "learning_rate": 0.00014201086956521742,
      "loss": 0.3862,
      "step": 2140
    },
    {
      "epoch": 5.842391304347826,
      "grad_norm": 4.252721786499023,
      "learning_rate": 0.00014173913043478262,
      "loss": 0.485,
      "step": 2150
    },
    {
      "epoch": 5.869565217391305,
      "grad_norm": 1.961559534072876,
      "learning_rate": 0.00014146739130434783,
      "loss": 0.4357,
      "step": 2160
    },
    {
      "epoch": 5.896739130434782,
      "grad_norm": 2.0232527256011963,
      "learning_rate": 0.00014119565217391304,
      "loss": 0.304,
      "step": 2170
    },
    {
      "epoch": 5.923913043478261,
      "grad_norm": 1.5802297592163086,
      "learning_rate": 0.00014092391304347827,
      "loss": 0.5613,
      "step": 2180
    },
    {
      "epoch": 5.951086956521739,
      "grad_norm": 18.941722869873047,
      "learning_rate": 0.00014065217391304348,
      "loss": 0.4981,
      "step": 2190
    },
    {
      "epoch": 5.978260869565218,
      "grad_norm": 1.5385347604751587,
      "learning_rate": 0.0001403804347826087,
      "loss": 0.2936,
      "step": 2200
    },
    {
      "epoch": 6.005434782608695,
      "grad_norm": 3.6977295875549316,
      "learning_rate": 0.00014010869565217392,
      "loss": 0.2835,
      "step": 2210
    },
    {
      "epoch": 6.032608695652174,
      "grad_norm": 3.231870651245117,
      "learning_rate": 0.00013983695652173913,
      "loss": 0.4705,
      "step": 2220
    },
    {
      "epoch": 6.059782608695652,
      "grad_norm": 1.7102296352386475,
      "learning_rate": 0.00013956521739130434,
      "loss": 0.3582,
      "step": 2230
    },
    {
      "epoch": 6.086956521739131,
      "grad_norm": 7.95699405670166,
      "learning_rate": 0.00013929347826086957,
      "loss": 0.1788,
      "step": 2240
    },
    {
      "epoch": 6.114130434782608,
      "grad_norm": 2.1693618297576904,
      "learning_rate": 0.0001390217391304348,
      "loss": 0.2433,
      "step": 2250
    },
    {
      "epoch": 6.141304347826087,
      "grad_norm": 3.4662938117980957,
      "learning_rate": 0.00013875,
      "loss": 0.3638,
      "step": 2260
    },
    {
      "epoch": 6.168478260869565,
      "grad_norm": 6.6598100662231445,
      "learning_rate": 0.00013847826086956525,
      "loss": 0.327,
      "step": 2270
    },
    {
      "epoch": 6.195652173913044,
      "grad_norm": 1.3970860242843628,
      "learning_rate": 0.00013820652173913045,
      "loss": 0.4944,
      "step": 2280
    },
    {
      "epoch": 6.2228260869565215,
      "grad_norm": 5.3334784507751465,
      "learning_rate": 0.00013793478260869566,
      "loss": 0.3757,
      "step": 2290
    },
    {
      "epoch": 6.25,
      "grad_norm": 2.2908012866973877,
      "learning_rate": 0.00013766304347826087,
      "loss": 0.3804,
      "step": 2300
    },
    {
      "epoch": 6.2771739130434785,
      "grad_norm": 4.000568866729736,
      "learning_rate": 0.0001373913043478261,
      "loss": 0.2835,
      "step": 2310
    },
    {
      "epoch": 6.304347826086957,
      "grad_norm": 2.290904998779297,
      "learning_rate": 0.0001371195652173913,
      "loss": 0.3051,
      "step": 2320
    },
    {
      "epoch": 6.331521739130435,
      "grad_norm": 3.2344436645507812,
      "learning_rate": 0.00013684782608695652,
      "loss": 0.4943,
      "step": 2330
    },
    {
      "epoch": 6.358695652173913,
      "grad_norm": 2.7100491523742676,
      "learning_rate": 0.00013657608695652175,
      "loss": 0.5415,
      "step": 2340
    },
    {
      "epoch": 6.385869565217392,
      "grad_norm": 10.062761306762695,
      "learning_rate": 0.00013630434782608696,
      "loss": 0.2902,
      "step": 2350
    },
    {
      "epoch": 6.413043478260869,
      "grad_norm": 5.563663959503174,
      "learning_rate": 0.00013603260869565217,
      "loss": 0.3563,
      "step": 2360
    },
    {
      "epoch": 6.440217391304348,
      "grad_norm": 3.639672040939331,
      "learning_rate": 0.0001357608695652174,
      "loss": 0.2688,
      "step": 2370
    },
    {
      "epoch": 6.467391304347826,
      "grad_norm": 1.3704603910446167,
      "learning_rate": 0.0001354891304347826,
      "loss": 0.1742,
      "step": 2380
    },
    {
      "epoch": 6.494565217391305,
      "grad_norm": 3.691384792327881,
      "learning_rate": 0.00013521739130434784,
      "loss": 0.2943,
      "step": 2390
    },
    {
      "epoch": 6.521739130434782,
      "grad_norm": 1.9917558431625366,
      "learning_rate": 0.00013494565217391305,
      "loss": 0.3255,
      "step": 2400
    },
    {
      "epoch": 6.548913043478261,
      "grad_norm": 9.302081108093262,
      "learning_rate": 0.00013467391304347828,
      "loss": 0.4319,
      "step": 2410
    },
    {
      "epoch": 6.576086956521739,
      "grad_norm": 8.726219177246094,
      "learning_rate": 0.0001344021739130435,
      "loss": 0.4201,
      "step": 2420
    },
    {
      "epoch": 6.603260869565218,
      "grad_norm": 8.96016788482666,
      "learning_rate": 0.0001341304347826087,
      "loss": 0.3209,
      "step": 2430
    },
    {
      "epoch": 6.630434782608695,
      "grad_norm": 10.168306350708008,
      "learning_rate": 0.00013385869565217393,
      "loss": 0.3968,
      "step": 2440
    },
    {
      "epoch": 6.657608695652174,
      "grad_norm": 1.4023574590682983,
      "learning_rate": 0.00013358695652173914,
      "loss": 0.315,
      "step": 2450
    },
    {
      "epoch": 6.684782608695652,
      "grad_norm": 2.687635660171509,
      "learning_rate": 0.00013331521739130435,
      "loss": 0.2673,
      "step": 2460
    },
    {
      "epoch": 6.711956521739131,
      "grad_norm": 6.7106781005859375,
      "learning_rate": 0.00013304347826086958,
      "loss": 0.4783,
      "step": 2470
    },
    {
      "epoch": 6.739130434782608,
      "grad_norm": 2.4227135181427,
      "learning_rate": 0.0001327717391304348,
      "loss": 0.28,
      "step": 2480
    },
    {
      "epoch": 6.766304347826087,
      "grad_norm": 12.169371604919434,
      "learning_rate": 0.0001325,
      "loss": 0.3266,
      "step": 2490
    },
    {
      "epoch": 6.793478260869565,
      "grad_norm": 4.557855606079102,
      "learning_rate": 0.00013222826086956523,
      "loss": 0.3881,
      "step": 2500
    },
    {
      "epoch": 6.820652173913043,
      "grad_norm": 1.2959233522415161,
      "learning_rate": 0.00013195652173913044,
      "loss": 0.3646,
      "step": 2510
    },
    {
      "epoch": 6.8478260869565215,
      "grad_norm": 3.01973295211792,
      "learning_rate": 0.00013168478260869564,
      "loss": 0.2635,
      "step": 2520
    },
    {
      "epoch": 6.875,
      "grad_norm": 1.149254560470581,
      "learning_rate": 0.00013141304347826088,
      "loss": 0.3091,
      "step": 2530
    },
    {
      "epoch": 6.9021739130434785,
      "grad_norm": 34.28078842163086,
      "learning_rate": 0.0001311413043478261,
      "loss": 0.4138,
      "step": 2540
    },
    {
      "epoch": 6.929347826086957,
      "grad_norm": 1.4650745391845703,
      "learning_rate": 0.00013086956521739132,
      "loss": 0.3154,
      "step": 2550
    },
    {
      "epoch": 6.956521739130435,
      "grad_norm": 5.812592029571533,
      "learning_rate": 0.00013059782608695653,
      "loss": 0.2535,
      "step": 2560
    },
    {
      "epoch": 6.983695652173913,
      "grad_norm": 3.3651766777038574,
      "learning_rate": 0.00013032608695652176,
      "loss": 0.3893,
      "step": 2570
    },
    {
      "epoch": 7.010869565217392,
      "grad_norm": 9.029683113098145,
      "learning_rate": 0.00013005434782608697,
      "loss": 0.3547,
      "step": 2580
    },
    {
      "epoch": 7.038043478260869,
      "grad_norm": 2.2411797046661377,
      "learning_rate": 0.00012978260869565218,
      "loss": 0.324,
      "step": 2590
    },
    {
      "epoch": 7.065217391304348,
      "grad_norm": 1.9626824855804443,
      "learning_rate": 0.0001295108695652174,
      "loss": 0.3242,
      "step": 2600
    },
    {
      "epoch": 7.092391304347826,
      "grad_norm": 3.0651676654815674,
      "learning_rate": 0.00012923913043478262,
      "loss": 0.2564,
      "step": 2610
    },
    {
      "epoch": 7.119565217391305,
      "grad_norm": 6.9355902671813965,
      "learning_rate": 0.00012896739130434782,
      "loss": 0.343,
      "step": 2620
    },
    {
      "epoch": 7.146739130434782,
      "grad_norm": 1.4689466953277588,
      "learning_rate": 0.00012869565217391303,
      "loss": 0.2729,
      "step": 2630
    },
    {
      "epoch": 7.173913043478261,
      "grad_norm": 2.4679768085479736,
      "learning_rate": 0.00012842391304347827,
      "loss": 0.1955,
      "step": 2640
    },
    {
      "epoch": 7.201086956521739,
      "grad_norm": 2.869736433029175,
      "learning_rate": 0.00012815217391304347,
      "loss": 0.2951,
      "step": 2650
    },
    {
      "epoch": 7.228260869565218,
      "grad_norm": 0.7786194682121277,
      "learning_rate": 0.00012788043478260868,
      "loss": 0.3084,
      "step": 2660
    },
    {
      "epoch": 7.255434782608695,
      "grad_norm": 1.8016538619995117,
      "learning_rate": 0.00012760869565217392,
      "loss": 0.3158,
      "step": 2670
    },
    {
      "epoch": 7.282608695652174,
      "grad_norm": 2.5064821243286133,
      "learning_rate": 0.00012733695652173915,
      "loss": 0.3779,
      "step": 2680
    },
    {
      "epoch": 7.309782608695652,
      "grad_norm": 1.5690547227859497,
      "learning_rate": 0.00012706521739130436,
      "loss": 0.2848,
      "step": 2690
    },
    {
      "epoch": 7.336956521739131,
      "grad_norm": 9.80601692199707,
      "learning_rate": 0.0001267934782608696,
      "loss": 0.2805,
      "step": 2700
    },
    {
      "epoch": 7.364130434782608,
      "grad_norm": 5.851068496704102,
      "learning_rate": 0.0001265217391304348,
      "loss": 0.2356,
      "step": 2710
    },
    {
      "epoch": 7.391304347826087,
      "grad_norm": 1.586344599723816,
      "learning_rate": 0.00012625,
      "loss": 0.2953,
      "step": 2720
    },
    {
      "epoch": 7.418478260869565,
      "grad_norm": 8.850166320800781,
      "learning_rate": 0.00012597826086956524,
      "loss": 0.2796,
      "step": 2730
    },
    {
      "epoch": 7.445652173913043,
      "grad_norm": 3.1630215644836426,
      "learning_rate": 0.00012570652173913045,
      "loss": 0.3529,
      "step": 2740
    },
    {
      "epoch": 7.4728260869565215,
      "grad_norm": 4.386247634887695,
      "learning_rate": 0.00012543478260869565,
      "loss": 0.3423,
      "step": 2750
    },
    {
      "epoch": 7.5,
      "grad_norm": 7.544158458709717,
      "learning_rate": 0.00012516304347826086,
      "loss": 0.3075,
      "step": 2760
    },
    {
      "epoch": 7.5271739130434785,
      "grad_norm": 1.77948796749115,
      "learning_rate": 0.0001248913043478261,
      "loss": 0.3272,
      "step": 2770
    },
    {
      "epoch": 7.554347826086957,
      "grad_norm": 1.5687086582183838,
      "learning_rate": 0.0001246195652173913,
      "loss": 0.2848,
      "step": 2780
    },
    {
      "epoch": 7.581521739130435,
      "grad_norm": 1.4625798463821411,
      "learning_rate": 0.0001243478260869565,
      "loss": 0.3831,
      "step": 2790
    },
    {
      "epoch": 7.608695652173913,
      "grad_norm": 1.2438244819641113,
      "learning_rate": 0.00012407608695652175,
      "loss": 0.323,
      "step": 2800
    },
    {
      "epoch": 7.635869565217392,
      "grad_norm": 1.0066083669662476,
      "learning_rate": 0.00012380434782608695,
      "loss": 0.1851,
      "step": 2810
    },
    {
      "epoch": 7.663043478260869,
      "grad_norm": 1.9281517267227173,
      "learning_rate": 0.0001235326086956522,
      "loss": 0.1823,
      "step": 2820
    },
    {
      "epoch": 7.690217391304348,
      "grad_norm": 3.524258852005005,
      "learning_rate": 0.0001232608695652174,
      "loss": 0.4488,
      "step": 2830
    },
    {
      "epoch": 7.717391304347826,
      "grad_norm": 0.8877074122428894,
      "learning_rate": 0.00012298913043478263,
      "loss": 0.3143,
      "step": 2840
    },
    {
      "epoch": 7.744565217391305,
      "grad_norm": 3.6359341144561768,
      "learning_rate": 0.00012271739130434784,
      "loss": 0.3277,
      "step": 2850
    },
    {
      "epoch": 7.771739130434782,
      "grad_norm": 5.1783881187438965,
      "learning_rate": 0.00012244565217391304,
      "loss": 0.3132,
      "step": 2860
    },
    {
      "epoch": 7.798913043478261,
      "grad_norm": 1.9305779933929443,
      "learning_rate": 0.00012217391304347828,
      "loss": 0.2567,
      "step": 2870
    },
    {
      "epoch": 7.826086956521739,
      "grad_norm": 2.0010836124420166,
      "learning_rate": 0.00012190217391304348,
      "loss": 0.3481,
      "step": 2880
    },
    {
      "epoch": 7.853260869565218,
      "grad_norm": 2.7570807933807373,
      "learning_rate": 0.00012163043478260869,
      "loss": 0.2289,
      "step": 2890
    },
    {
      "epoch": 7.880434782608695,
      "grad_norm": 2.6967813968658447,
      "learning_rate": 0.00012135869565217393,
      "loss": 0.2359,
      "step": 2900
    },
    {
      "epoch": 7.907608695652174,
      "grad_norm": 1.3921757936477661,
      "learning_rate": 0.00012108695652173913,
      "loss": 0.3196,
      "step": 2910
    },
    {
      "epoch": 7.934782608695652,
      "grad_norm": 1.4234827756881714,
      "learning_rate": 0.00012081521739130434,
      "loss": 0.3057,
      "step": 2920
    },
    {
      "epoch": 7.961956521739131,
      "grad_norm": 1.902860403060913,
      "learning_rate": 0.00012054347826086957,
      "loss": 0.3603,
      "step": 2930
    },
    {
      "epoch": 7.989130434782608,
      "grad_norm": 1.6862481832504272,
      "learning_rate": 0.0001202717391304348,
      "loss": 0.4322,
      "step": 2940
    },
    {
      "epoch": 8.016304347826088,
      "grad_norm": 14.704113960266113,
      "learning_rate": 0.00012,
      "loss": 0.2783,
      "step": 2950
    },
    {
      "epoch": 8.043478260869565,
      "grad_norm": 1.8711166381835938,
      "learning_rate": 0.00011972826086956524,
      "loss": 0.2736,
      "step": 2960
    },
    {
      "epoch": 8.070652173913043,
      "grad_norm": 1.1817588806152344,
      "learning_rate": 0.00011945652173913044,
      "loss": 0.2849,
      "step": 2970
    },
    {
      "epoch": 8.097826086956522,
      "grad_norm": 8.228872299194336,
      "learning_rate": 0.00011918478260869565,
      "loss": 0.3712,
      "step": 2980
    },
    {
      "epoch": 8.125,
      "grad_norm": 4.3462958335876465,
      "learning_rate": 0.00011891304347826087,
      "loss": 0.2878,
      "step": 2990
    },
    {
      "epoch": 8.152173913043478,
      "grad_norm": 6.948309421539307,
      "learning_rate": 0.0001186413043478261,
      "loss": 0.2386,
      "step": 3000
    },
    {
      "epoch": 8.179347826086957,
      "grad_norm": 1.2213492393493652,
      "learning_rate": 0.00011836956521739131,
      "loss": 0.1835,
      "step": 3010
    },
    {
      "epoch": 8.206521739130435,
      "grad_norm": 3.541748523712158,
      "learning_rate": 0.00011809782608695652,
      "loss": 0.2849,
      "step": 3020
    },
    {
      "epoch": 8.233695652173912,
      "grad_norm": 2.868210554122925,
      "learning_rate": 0.00011782608695652176,
      "loss": 0.2908,
      "step": 3030
    },
    {
      "epoch": 8.26086956521739,
      "grad_norm": 1.6864322423934937,
      "learning_rate": 0.00011755434782608696,
      "loss": 0.3343,
      "step": 3040
    },
    {
      "epoch": 8.28804347826087,
      "grad_norm": 1.566978931427002,
      "learning_rate": 0.00011728260869565217,
      "loss": 0.2765,
      "step": 3050
    },
    {
      "epoch": 8.315217391304348,
      "grad_norm": 1.7020649909973145,
      "learning_rate": 0.0001170108695652174,
      "loss": 0.3236,
      "step": 3060
    },
    {
      "epoch": 8.342391304347826,
      "grad_norm": 3.371530294418335,
      "learning_rate": 0.00011673913043478261,
      "loss": 0.2398,
      "step": 3070
    },
    {
      "epoch": 8.369565217391305,
      "grad_norm": 1.4004853963851929,
      "learning_rate": 0.00011646739130434783,
      "loss": 0.2463,
      "step": 3080
    },
    {
      "epoch": 8.396739130434783,
      "grad_norm": 5.831719875335693,
      "learning_rate": 0.00011619565217391304,
      "loss": 0.2835,
      "step": 3090
    },
    {
      "epoch": 8.423913043478262,
      "grad_norm": 1.2850192785263062,
      "learning_rate": 0.00011592391304347827,
      "loss": 0.2218,
      "step": 3100
    },
    {
      "epoch": 8.451086956521738,
      "grad_norm": 2.134044885635376,
      "learning_rate": 0.00011565217391304348,
      "loss": 0.2569,
      "step": 3110
    },
    {
      "epoch": 8.478260869565217,
      "grad_norm": 2.6843695640563965,
      "learning_rate": 0.00011538043478260869,
      "loss": 0.3062,
      "step": 3120
    },
    {
      "epoch": 8.505434782608695,
      "grad_norm": 7.3289642333984375,
      "learning_rate": 0.00011510869565217392,
      "loss": 0.2877,
      "step": 3130
    },
    {
      "epoch": 8.532608695652174,
      "grad_norm": 2.866011381149292,
      "learning_rate": 0.00011483695652173913,
      "loss": 0.241,
      "step": 3140
    },
    {
      "epoch": 8.559782608695652,
      "grad_norm": 9.470979690551758,
      "learning_rate": 0.00011456521739130435,
      "loss": 0.2924,
      "step": 3150
    },
    {
      "epoch": 8.58695652173913,
      "grad_norm": 6.673827648162842,
      "learning_rate": 0.00011429347826086959,
      "loss": 0.2589,
      "step": 3160
    },
    {
      "epoch": 8.61413043478261,
      "grad_norm": 1.1500053405761719,
      "learning_rate": 0.00011402173913043479,
      "loss": 0.3466,
      "step": 3170
    },
    {
      "epoch": 8.641304347826086,
      "grad_norm": 1.428128957748413,
      "learning_rate": 0.00011375,
      "loss": 0.2986,
      "step": 3180
    },
    {
      "epoch": 8.668478260869565,
      "grad_norm": 3.2889647483825684,
      "learning_rate": 0.00011347826086956523,
      "loss": 0.2929,
      "step": 3190
    },
    {
      "epoch": 8.695652173913043,
      "grad_norm": 8.984533309936523,
      "learning_rate": 0.00011320652173913044,
      "loss": 0.2592,
      "step": 3200
    },
    {
      "epoch": 8.722826086956522,
      "grad_norm": 1.7902352809906006,
      "learning_rate": 0.00011293478260869565,
      "loss": 0.2681,
      "step": 3210
    },
    {
      "epoch": 8.75,
      "grad_norm": 2.0383331775665283,
      "learning_rate": 0.00011266304347826087,
      "loss": 0.2087,
      "step": 3220
    },
    {
      "epoch": 8.777173913043478,
      "grad_norm": 3.041259288787842,
      "learning_rate": 0.0001123913043478261,
      "loss": 0.2577,
      "step": 3230
    },
    {
      "epoch": 8.804347826086957,
      "grad_norm": 4.185965538024902,
      "learning_rate": 0.00011211956521739131,
      "loss": 0.2824,
      "step": 3240
    },
    {
      "epoch": 8.831521739130435,
      "grad_norm": 2.1872899532318115,
      "learning_rate": 0.00011184782608695652,
      "loss": 0.3418,
      "step": 3250
    },
    {
      "epoch": 8.858695652173914,
      "grad_norm": 1.4757455587387085,
      "learning_rate": 0.00011157608695652175,
      "loss": 0.2457,
      "step": 3260
    },
    {
      "epoch": 8.88586956521739,
      "grad_norm": 1.3251115083694458,
      "learning_rate": 0.00011130434782608696,
      "loss": 0.2237,
      "step": 3270
    },
    {
      "epoch": 8.91304347826087,
      "grad_norm": 1.6889257431030273,
      "learning_rate": 0.00011103260869565217,
      "loss": 0.2758,
      "step": 3280
    },
    {
      "epoch": 8.940217391304348,
      "grad_norm": 1.354097843170166,
      "learning_rate": 0.0001107608695652174,
      "loss": 0.2161,
      "step": 3290
    },
    {
      "epoch": 8.967391304347826,
      "grad_norm": 6.068423271179199,
      "learning_rate": 0.00011048913043478262,
      "loss": 0.3649,
      "step": 3300
    },
    {
      "epoch": 8.994565217391305,
      "grad_norm": 20.827205657958984,
      "learning_rate": 0.00011021739130434783,
      "loss": 0.3735,
      "step": 3310
    },
    {
      "epoch": 9.021739130434783,
      "grad_norm": 0.9171772599220276,
      "learning_rate": 0.00010994565217391304,
      "loss": 0.2143,
      "step": 3320
    },
    {
      "epoch": 9.048913043478262,
      "grad_norm": 19.106361389160156,
      "learning_rate": 0.00010967391304347827,
      "loss": 0.3523,
      "step": 3330
    },
    {
      "epoch": 9.076086956521738,
      "grad_norm": 1.619290828704834,
      "learning_rate": 0.00010940217391304348,
      "loss": 0.156,
      "step": 3340
    },
    {
      "epoch": 9.103260869565217,
      "grad_norm": 0.8237000703811646,
      "learning_rate": 0.00010913043478260869,
      "loss": 0.2509,
      "step": 3350
    },
    {
      "epoch": 9.130434782608695,
      "grad_norm": 2.993469476699829,
      "learning_rate": 0.00010885869565217392,
      "loss": 0.2167,
      "step": 3360
    },
    {
      "epoch": 9.157608695652174,
      "grad_norm": 2.2952449321746826,
      "learning_rate": 0.00010858695652173914,
      "loss": 0.2799,
      "step": 3370
    },
    {
      "epoch": 9.184782608695652,
      "grad_norm": 1.2718498706817627,
      "learning_rate": 0.00010831521739130435,
      "loss": 0.258,
      "step": 3380
    },
    {
      "epoch": 9.21195652173913,
      "grad_norm": 2.124152660369873,
      "learning_rate": 0.00010804347826086958,
      "loss": 0.2345,
      "step": 3390
    },
    {
      "epoch": 9.23913043478261,
      "grad_norm": 2.9851438999176025,
      "learning_rate": 0.00010777173913043479,
      "loss": 0.289,
      "step": 3400
    },
    {
      "epoch": 9.266304347826088,
      "grad_norm": 2.989682197570801,
      "learning_rate": 0.0001075,
      "loss": 0.3008,
      "step": 3410
    },
    {
      "epoch": 9.293478260869565,
      "grad_norm": 8.398653030395508,
      "learning_rate": 0.00010722826086956523,
      "loss": 0.2792,
      "step": 3420
    },
    {
      "epoch": 9.320652173913043,
      "grad_norm": 5.738563060760498,
      "learning_rate": 0.00010695652173913044,
      "loss": 0.2096,
      "step": 3430
    },
    {
      "epoch": 9.347826086956522,
      "grad_norm": 1.7963320016860962,
      "learning_rate": 0.00010668478260869566,
      "loss": 0.2211,
      "step": 3440
    },
    {
      "epoch": 9.375,
      "grad_norm": 14.687253952026367,
      "learning_rate": 0.00010641304347826087,
      "loss": 0.3004,
      "step": 3450
    },
    {
      "epoch": 9.402173913043478,
      "grad_norm": 3.816563367843628,
      "learning_rate": 0.0001061413043478261,
      "loss": 0.2425,
      "step": 3460
    },
    {
      "epoch": 9.429347826086957,
      "grad_norm": 2.019951343536377,
      "learning_rate": 0.00010586956521739131,
      "loss": 0.1882,
      "step": 3470
    },
    {
      "epoch": 9.456521739130435,
      "grad_norm": 12.928173065185547,
      "learning_rate": 0.00010559782608695652,
      "loss": 0.2862,
      "step": 3480
    },
    {
      "epoch": 9.483695652173912,
      "grad_norm": 0.9002472758293152,
      "learning_rate": 0.00010532608695652175,
      "loss": 0.1987,
      "step": 3490
    },
    {
      "epoch": 9.51086956521739,
      "grad_norm": 5.123678207397461,
      "learning_rate": 0.00010505434782608696,
      "loss": 0.2964,
      "step": 3500
    },
    {
      "epoch": 9.53804347826087,
      "grad_norm": 1.375407099723816,
      "learning_rate": 0.00010478260869565218,
      "loss": 0.3089,
      "step": 3510
    },
    {
      "epoch": 9.565217391304348,
      "grad_norm": 1.181014895439148,
      "learning_rate": 0.00010451086956521741,
      "loss": 0.2361,
      "step": 3520
    },
    {
      "epoch": 9.592391304347826,
      "grad_norm": 1.3668534755706787,
      "learning_rate": 0.00010423913043478262,
      "loss": 0.2408,
      "step": 3530
    },
    {
      "epoch": 9.619565217391305,
      "grad_norm": 1.805080533027649,
      "learning_rate": 0.00010396739130434783,
      "loss": 0.2328,
      "step": 3540
    },
    {
      "epoch": 9.646739130434783,
      "grad_norm": 8.97352409362793,
      "learning_rate": 0.00010369565217391303,
      "loss": 0.2476,
      "step": 3550
    },
    {
      "epoch": 9.673913043478262,
      "grad_norm": 1.7416234016418457,
      "learning_rate": 0.00010342391304347827,
      "loss": 0.2309,
      "step": 3560
    },
    {
      "epoch": 9.701086956521738,
      "grad_norm": 11.1276273727417,
      "learning_rate": 0.00010315217391304348,
      "loss": 0.3266,
      "step": 3570
    },
    {
      "epoch": 9.728260869565217,
      "grad_norm": 4.1995134353637695,
      "learning_rate": 0.0001028804347826087,
      "loss": 0.3412,
      "step": 3580
    },
    {
      "epoch": 9.755434782608695,
      "grad_norm": 2.3300395011901855,
      "learning_rate": 0.00010260869565217393,
      "loss": 0.3533,
      "step": 3590
    },
    {
      "epoch": 9.782608695652174,
      "grad_norm": 4.010826110839844,
      "learning_rate": 0.00010233695652173914,
      "loss": 0.2828,
      "step": 3600
    },
    {
      "epoch": 9.809782608695652,
      "grad_norm": 3.2000603675842285,
      "learning_rate": 0.00010206521739130435,
      "loss": 0.2213,
      "step": 3610
    },
    {
      "epoch": 9.83695652173913,
      "grad_norm": 3.5600364208221436,
      "learning_rate": 0.00010179347826086958,
      "loss": 0.247,
      "step": 3620
    },
    {
      "epoch": 9.86413043478261,
      "grad_norm": 9.528359413146973,
      "learning_rate": 0.00010152173913043479,
      "loss": 0.2764,
      "step": 3630
    },
    {
      "epoch": 9.891304347826086,
      "grad_norm": 1.5969452857971191,
      "learning_rate": 0.00010125,
      "loss": 0.2547,
      "step": 3640
    },
    {
      "epoch": 9.918478260869565,
      "grad_norm": 14.914989471435547,
      "learning_rate": 0.00010097826086956523,
      "loss": 0.2355,
      "step": 3650
    },
    {
      "epoch": 9.945652173913043,
      "grad_norm": 1.3883848190307617,
      "learning_rate": 0.00010070652173913045,
      "loss": 0.2152,
      "step": 3660
    },
    {
      "epoch": 9.972826086956522,
      "grad_norm": 6.883042335510254,
      "learning_rate": 0.00010043478260869566,
      "loss": 0.202,
      "step": 3670
    },
    {
      "epoch": 10.0,
      "grad_norm": 11.771819114685059,
      "learning_rate": 0.00010016304347826086,
      "loss": 0.2747,
      "step": 3680
    },
    {
      "epoch": 10.027173913043478,
      "grad_norm": 3.224973440170288,
      "learning_rate": 9.989130434782608e-05,
      "loss": 0.1661,
      "step": 3690
    },
    {
      "epoch": 10.054347826086957,
      "grad_norm": 2.3976151943206787,
      "learning_rate": 9.96195652173913e-05,
      "loss": 0.21,
      "step": 3700
    },
    {
      "epoch": 10.081521739130435,
      "grad_norm": 2.5042848587036133,
      "learning_rate": 9.934782608695653e-05,
      "loss": 0.2206,
      "step": 3710
    },
    {
      "epoch": 10.108695652173912,
      "grad_norm": 1.5640621185302734,
      "learning_rate": 9.907608695652175e-05,
      "loss": 0.2395,
      "step": 3720
    },
    {
      "epoch": 10.13586956521739,
      "grad_norm": 1.637947916984558,
      "learning_rate": 9.880434782608697e-05,
      "loss": 0.2636,
      "step": 3730
    },
    {
      "epoch": 10.16304347826087,
      "grad_norm": 1.342736005783081,
      "learning_rate": 9.853260869565218e-05,
      "loss": 0.2006,
      "step": 3740
    },
    {
      "epoch": 10.190217391304348,
      "grad_norm": 2.1386475563049316,
      "learning_rate": 9.82608695652174e-05,
      "loss": 0.2393,
      "step": 3750
    },
    {
      "epoch": 10.217391304347826,
      "grad_norm": 4.876583576202393,
      "learning_rate": 9.798913043478262e-05,
      "loss": 0.1927,
      "step": 3760
    },
    {
      "epoch": 10.244565217391305,
      "grad_norm": 3.083526611328125,
      "learning_rate": 9.771739130434782e-05,
      "loss": 0.26,
      "step": 3770
    },
    {
      "epoch": 10.271739130434783,
      "grad_norm": 2.0032665729522705,
      "learning_rate": 9.744565217391305e-05,
      "loss": 0.225,
      "step": 3780
    },
    {
      "epoch": 10.298913043478262,
      "grad_norm": 1.3532359600067139,
      "learning_rate": 9.717391304347827e-05,
      "loss": 0.2871,
      "step": 3790
    },
    {
      "epoch": 10.326086956521738,
      "grad_norm": 1.9580655097961426,
      "learning_rate": 9.690217391304349e-05,
      "loss": 0.2387,
      "step": 3800
    },
    {
      "epoch": 10.353260869565217,
      "grad_norm": 6.369017124176025,
      "learning_rate": 9.663043478260871e-05,
      "loss": 0.1999,
      "step": 3810
    },
    {
      "epoch": 10.380434782608695,
      "grad_norm": 1.5211825370788574,
      "learning_rate": 9.635869565217391e-05,
      "loss": 0.3055,
      "step": 3820
    },
    {
      "epoch": 10.407608695652174,
      "grad_norm": 3.922172784805298,
      "learning_rate": 9.608695652173914e-05,
      "loss": 0.2215,
      "step": 3830
    },
    {
      "epoch": 10.434782608695652,
      "grad_norm": 1.5561213493347168,
      "learning_rate": 9.581521739130436e-05,
      "loss": 0.2213,
      "step": 3840
    },
    {
      "epoch": 10.46195652173913,
      "grad_norm": 5.663263320922852,
      "learning_rate": 9.554347826086956e-05,
      "loss": 0.1665,
      "step": 3850
    },
    {
      "epoch": 10.48913043478261,
      "grad_norm": 1.4248332977294922,
      "learning_rate": 9.527173913043478e-05,
      "loss": 0.23,
      "step": 3860
    },
    {
      "epoch": 10.516304347826086,
      "grad_norm": 1.822076678276062,
      "learning_rate": 9.5e-05,
      "loss": 0.2372,
      "step": 3870
    },
    {
      "epoch": 10.543478260869565,
      "grad_norm": 1.0158008337020874,
      "learning_rate": 9.472826086956523e-05,
      "loss": 0.2854,
      "step": 3880
    },
    {
      "epoch": 10.570652173913043,
      "grad_norm": 1.1765660047531128,
      "learning_rate": 9.445652173913045e-05,
      "loss": 0.2734,
      "step": 3890
    },
    {
      "epoch": 10.597826086956522,
      "grad_norm": 1.4316246509552002,
      "learning_rate": 9.418478260869565e-05,
      "loss": 0.212,
      "step": 3900
    },
    {
      "epoch": 10.625,
      "grad_norm": 1.4040528535842896,
      "learning_rate": 9.391304347826087e-05,
      "loss": 0.2504,
      "step": 3910
    },
    {
      "epoch": 10.652173913043478,
      "grad_norm": 2.726533889770508,
      "learning_rate": 9.364130434782608e-05,
      "loss": 0.2332,
      "step": 3920
    },
    {
      "epoch": 10.679347826086957,
      "grad_norm": 10.504237174987793,
      "learning_rate": 9.33695652173913e-05,
      "loss": 0.1623,
      "step": 3930
    },
    {
      "epoch": 10.706521739130435,
      "grad_norm": 1.158315896987915,
      "learning_rate": 9.309782608695652e-05,
      "loss": 0.2645,
      "step": 3940
    },
    {
      "epoch": 10.733695652173914,
      "grad_norm": 2.802513837814331,
      "learning_rate": 9.282608695652174e-05,
      "loss": 0.2815,
      "step": 3950
    },
    {
      "epoch": 10.76086956521739,
      "grad_norm": 1.6588692665100098,
      "learning_rate": 9.255434782608697e-05,
      "loss": 0.2917,
      "step": 3960
    },
    {
      "epoch": 10.78804347826087,
      "grad_norm": 0.8136271834373474,
      "learning_rate": 9.228260869565217e-05,
      "loss": 0.2632,
      "step": 3970
    },
    {
      "epoch": 10.815217391304348,
      "grad_norm": 2.4332165718078613,
      "learning_rate": 9.20108695652174e-05,
      "loss": 0.2539,
      "step": 3980
    },
    {
      "epoch": 10.842391304347826,
      "grad_norm": 5.0206146240234375,
      "learning_rate": 9.173913043478261e-05,
      "loss": 0.2302,
      "step": 3990
    },
    {
      "epoch": 10.869565217391305,
      "grad_norm": 4.095597743988037,
      "learning_rate": 9.146739130434782e-05,
      "loss": 0.2202,
      "step": 4000
    },
    {
      "epoch": 10.896739130434783,
      "grad_norm": 9.396286010742188,
      "learning_rate": 9.119565217391304e-05,
      "loss": 0.1915,
      "step": 4010
    },
    {
      "epoch": 10.923913043478262,
      "grad_norm": 3.024440050125122,
      "learning_rate": 9.092391304347828e-05,
      "loss": 0.2536,
      "step": 4020
    },
    {
      "epoch": 10.951086956521738,
      "grad_norm": 2.8446810245513916,
      "learning_rate": 9.065217391304348e-05,
      "loss": 0.2853,
      "step": 4030
    },
    {
      "epoch": 10.978260869565217,
      "grad_norm": 5.759705543518066,
      "learning_rate": 9.03804347826087e-05,
      "loss": 0.2227,
      "step": 4040
    },
    {
      "epoch": 11.005434782608695,
      "grad_norm": 2.0621554851531982,
      "learning_rate": 9.010869565217391e-05,
      "loss": 0.2698,
      "step": 4050
    },
    {
      "epoch": 11.032608695652174,
      "grad_norm": 2.1126811504364014,
      "learning_rate": 8.983695652173913e-05,
      "loss": 0.1923,
      "step": 4060
    },
    {
      "epoch": 11.059782608695652,
      "grad_norm": 1.355320692062378,
      "learning_rate": 8.956521739130435e-05,
      "loss": 0.1664,
      "step": 4070
    },
    {
      "epoch": 11.08695652173913,
      "grad_norm": 2.940424919128418,
      "learning_rate": 8.929347826086956e-05,
      "loss": 0.2121,
      "step": 4080
    },
    {
      "epoch": 11.11413043478261,
      "grad_norm": 2.237309694290161,
      "learning_rate": 8.90217391304348e-05,
      "loss": 0.2144,
      "step": 4090
    },
    {
      "epoch": 11.141304347826088,
      "grad_norm": 1.9029005765914917,
      "learning_rate": 8.875e-05,
      "loss": 0.2592,
      "step": 4100
    },
    {
      "epoch": 11.168478260869565,
      "grad_norm": 1.1923127174377441,
      "learning_rate": 8.847826086956522e-05,
      "loss": 0.2279,
      "step": 4110
    },
    {
      "epoch": 11.195652173913043,
      "grad_norm": 1.0567235946655273,
      "learning_rate": 8.820652173913044e-05,
      "loss": 0.2154,
      "step": 4120
    },
    {
      "epoch": 11.222826086956522,
      "grad_norm": 2.2734181880950928,
      "learning_rate": 8.793478260869565e-05,
      "loss": 0.2167,
      "step": 4130
    },
    {
      "epoch": 11.25,
      "grad_norm": 0.994953989982605,
      "learning_rate": 8.766304347826087e-05,
      "loss": 0.2206,
      "step": 4140
    },
    {
      "epoch": 11.277173913043478,
      "grad_norm": 1.4027191400527954,
      "learning_rate": 8.739130434782609e-05,
      "loss": 0.2292,
      "step": 4150
    },
    {
      "epoch": 11.304347826086957,
      "grad_norm": 13.371670722961426,
      "learning_rate": 8.711956521739131e-05,
      "loss": 0.2189,
      "step": 4160
    },
    {
      "epoch": 11.331521739130435,
      "grad_norm": 3.7466516494750977,
      "learning_rate": 8.684782608695653e-05,
      "loss": 0.2518,
      "step": 4170
    },
    {
      "epoch": 11.358695652173912,
      "grad_norm": 1.525625467300415,
      "learning_rate": 8.657608695652174e-05,
      "loss": 0.2329,
      "step": 4180
    },
    {
      "epoch": 11.38586956521739,
      "grad_norm": 3.525282621383667,
      "learning_rate": 8.630434782608696e-05,
      "loss": 0.2418,
      "step": 4190
    },
    {
      "epoch": 11.41304347826087,
      "grad_norm": 2.3722338676452637,
      "learning_rate": 8.603260869565217e-05,
      "loss": 0.1787,
      "step": 4200
    },
    {
      "epoch": 11.440217391304348,
      "grad_norm": 5.109384059906006,
      "learning_rate": 8.576086956521739e-05,
      "loss": 0.2247,
      "step": 4210
    },
    {
      "epoch": 11.467391304347826,
      "grad_norm": 14.299287796020508,
      "learning_rate": 8.548913043478261e-05,
      "loss": 0.2547,
      "step": 4220
    },
    {
      "epoch": 11.494565217391305,
      "grad_norm": 2.1122796535491943,
      "learning_rate": 8.521739130434783e-05,
      "loss": 0.2175,
      "step": 4230
    },
    {
      "epoch": 11.521739130434783,
      "grad_norm": 1.0514581203460693,
      "learning_rate": 8.494565217391305e-05,
      "loss": 0.2085,
      "step": 4240
    },
    {
      "epoch": 11.548913043478262,
      "grad_norm": 2.9529237747192383,
      "learning_rate": 8.467391304347827e-05,
      "loss": 0.2159,
      "step": 4250
    },
    {
      "epoch": 11.576086956521738,
      "grad_norm": 5.29519510269165,
      "learning_rate": 8.440217391304348e-05,
      "loss": 0.2154,
      "step": 4260
    },
    {
      "epoch": 11.603260869565217,
      "grad_norm": 6.356170177459717,
      "learning_rate": 8.41304347826087e-05,
      "loss": 0.2232,
      "step": 4270
    },
    {
      "epoch": 11.630434782608695,
      "grad_norm": 2.2664709091186523,
      "learning_rate": 8.385869565217391e-05,
      "loss": 0.2244,
      "step": 4280
    },
    {
      "epoch": 11.657608695652174,
      "grad_norm": 2.4752652645111084,
      "learning_rate": 8.358695652173913e-05,
      "loss": 0.1982,
      "step": 4290
    },
    {
      "epoch": 11.684782608695652,
      "grad_norm": 1.434104084968567,
      "learning_rate": 8.331521739130435e-05,
      "loss": 0.1844,
      "step": 4300
    },
    {
      "epoch": 11.71195652173913,
      "grad_norm": 3.8599348068237305,
      "learning_rate": 8.304347826086957e-05,
      "loss": 0.3335,
      "step": 4310
    },
    {
      "epoch": 11.73913043478261,
      "grad_norm": 3.294398546218872,
      "learning_rate": 8.277173913043479e-05,
      "loss": 0.2266,
      "step": 4320
    },
    {
      "epoch": 11.766304347826086,
      "grad_norm": 3.1694960594177246,
      "learning_rate": 8.25e-05,
      "loss": 0.197,
      "step": 4330
    },
    {
      "epoch": 11.793478260869565,
      "grad_norm": 1.8283357620239258,
      "learning_rate": 8.222826086956522e-05,
      "loss": 0.2374,
      "step": 4340
    },
    {
      "epoch": 11.820652173913043,
      "grad_norm": 1.5861737728118896,
      "learning_rate": 8.195652173913044e-05,
      "loss": 0.227,
      "step": 4350
    },
    {
      "epoch": 11.847826086956522,
      "grad_norm": 4.43186616897583,
      "learning_rate": 8.168478260869565e-05,
      "loss": 0.2573,
      "step": 4360
    },
    {
      "epoch": 11.875,
      "grad_norm": 6.122562885284424,
      "learning_rate": 8.141304347826087e-05,
      "loss": 0.2703,
      "step": 4370
    },
    {
      "epoch": 11.902173913043478,
      "grad_norm": 4.162418842315674,
      "learning_rate": 8.114130434782609e-05,
      "loss": 0.242,
      "step": 4380
    },
    {
      "epoch": 11.929347826086957,
      "grad_norm": 1.2153735160827637,
      "learning_rate": 8.086956521739131e-05,
      "loss": 0.2483,
      "step": 4390
    },
    {
      "epoch": 11.956521739130435,
      "grad_norm": 2.270674705505371,
      "learning_rate": 8.059782608695653e-05,
      "loss": 0.185,
      "step": 4400
    },
    {
      "epoch": 11.983695652173914,
      "grad_norm": 1.6024563312530518,
      "learning_rate": 8.032608695652174e-05,
      "loss": 0.1807,
      "step": 4410
    },
    {
      "epoch": 12.01086956521739,
      "grad_norm": 1.7901322841644287,
      "learning_rate": 8.005434782608696e-05,
      "loss": 0.2173,
      "step": 4420
    },
    {
      "epoch": 12.03804347826087,
      "grad_norm": 1.560037612915039,
      "learning_rate": 7.978260869565217e-05,
      "loss": 0.2015,
      "step": 4430
    },
    {
      "epoch": 12.065217391304348,
      "grad_norm": 2.8490641117095947,
      "learning_rate": 7.951086956521739e-05,
      "loss": 0.1983,
      "step": 4440
    },
    {
      "epoch": 12.092391304347826,
      "grad_norm": 3.509800672531128,
      "learning_rate": 7.923913043478262e-05,
      "loss": 0.2118,
      "step": 4450
    },
    {
      "epoch": 12.119565217391305,
      "grad_norm": 1.4840331077575684,
      "learning_rate": 7.896739130434783e-05,
      "loss": 0.1911,
      "step": 4460
    },
    {
      "epoch": 12.146739130434783,
      "grad_norm": 1.1134719848632812,
      "learning_rate": 7.869565217391305e-05,
      "loss": 0.22,
      "step": 4470
    },
    {
      "epoch": 12.173913043478262,
      "grad_norm": 1.7103850841522217,
      "learning_rate": 7.842391304347827e-05,
      "loss": 0.1927,
      "step": 4480
    },
    {
      "epoch": 12.201086956521738,
      "grad_norm": 8.577360153198242,
      "learning_rate": 7.815217391304348e-05,
      "loss": 0.2094,
      "step": 4490
    },
    {
      "epoch": 12.228260869565217,
      "grad_norm": 1.1362625360488892,
      "learning_rate": 7.78804347826087e-05,
      "loss": 0.1918,
      "step": 4500
    },
    {
      "epoch": 12.255434782608695,
      "grad_norm": 1.9068872928619385,
      "learning_rate": 7.76086956521739e-05,
      "loss": 0.1864,
      "step": 4510
    },
    {
      "epoch": 12.282608695652174,
      "grad_norm": 3.244351387023926,
      "learning_rate": 7.733695652173914e-05,
      "loss": 0.2072,
      "step": 4520
    },
    {
      "epoch": 12.309782608695652,
      "grad_norm": 5.049403667449951,
      "learning_rate": 7.706521739130436e-05,
      "loss": 0.2431,
      "step": 4530
    },
    {
      "epoch": 12.33695652173913,
      "grad_norm": 1.7173881530761719,
      "learning_rate": 7.679347826086957e-05,
      "loss": 0.1925,
      "step": 4540
    },
    {
      "epoch": 12.36413043478261,
      "grad_norm": 9.957462310791016,
      "learning_rate": 7.652173913043479e-05,
      "loss": 0.2498,
      "step": 4550
    },
    {
      "epoch": 12.391304347826088,
      "grad_norm": 2.186699628829956,
      "learning_rate": 7.625e-05,
      "loss": 0.2032,
      "step": 4560
    },
    {
      "epoch": 12.418478260869565,
      "grad_norm": 7.680134296417236,
      "learning_rate": 7.597826086956522e-05,
      "loss": 0.2152,
      "step": 4570
    },
    {
      "epoch": 12.445652173913043,
      "grad_norm": 6.864403247833252,
      "learning_rate": 7.570652173913044e-05,
      "loss": 0.2023,
      "step": 4580
    },
    {
      "epoch": 12.472826086956522,
      "grad_norm": 1.2186387777328491,
      "learning_rate": 7.543478260869566e-05,
      "loss": 0.2327,
      "step": 4590
    },
    {
      "epoch": 12.5,
      "grad_norm": 2.9010627269744873,
      "learning_rate": 7.516304347826088e-05,
      "loss": 0.2342,
      "step": 4600
    },
    {
      "epoch": 12.527173913043478,
      "grad_norm": 2.128976583480835,
      "learning_rate": 7.489130434782609e-05,
      "loss": 0.2572,
      "step": 4610
    },
    {
      "epoch": 12.554347826086957,
      "grad_norm": 3.0941994190216064,
      "learning_rate": 7.461956521739131e-05,
      "loss": 0.2239,
      "step": 4620
    },
    {
      "epoch": 12.581521739130435,
      "grad_norm": 2.2055442333221436,
      "learning_rate": 7.434782608695653e-05,
      "loss": 0.1991,
      "step": 4630
    },
    {
      "epoch": 12.608695652173914,
      "grad_norm": 1.2069342136383057,
      "learning_rate": 7.407608695652174e-05,
      "loss": 0.174,
      "step": 4640
    },
    {
      "epoch": 12.63586956521739,
      "grad_norm": 3.9608683586120605,
      "learning_rate": 7.380434782608696e-05,
      "loss": 0.2199,
      "step": 4650
    },
    {
      "epoch": 12.66304347826087,
      "grad_norm": 2.436343193054199,
      "learning_rate": 7.353260869565218e-05,
      "loss": 0.2233,
      "step": 4660
    },
    {
      "epoch": 12.690217391304348,
      "grad_norm": 2.554939031600952,
      "learning_rate": 7.32608695652174e-05,
      "loss": 0.2647,
      "step": 4670
    },
    {
      "epoch": 12.717391304347826,
      "grad_norm": 1.2238905429840088,
      "learning_rate": 7.298913043478262e-05,
      "loss": 0.2148,
      "step": 4680
    },
    {
      "epoch": 12.744565217391305,
      "grad_norm": 7.779078006744385,
      "learning_rate": 7.271739130434783e-05,
      "loss": 0.247,
      "step": 4690
    },
    {
      "epoch": 12.771739130434783,
      "grad_norm": 2.9675426483154297,
      "learning_rate": 7.244565217391305e-05,
      "loss": 0.1855,
      "step": 4700
    },
    {
      "epoch": 12.798913043478262,
      "grad_norm": 2.029994010925293,
      "learning_rate": 7.217391304347827e-05,
      "loss": 0.2065,
      "step": 4710
    },
    {
      "epoch": 12.826086956521738,
      "grad_norm": 3.552295446395874,
      "learning_rate": 7.190217391304348e-05,
      "loss": 0.2772,
      "step": 4720
    },
    {
      "epoch": 12.853260869565217,
      "grad_norm": 3.2538764476776123,
      "learning_rate": 7.16304347826087e-05,
      "loss": 0.1966,
      "step": 4730
    },
    {
      "epoch": 12.880434782608695,
      "grad_norm": 1.3889268636703491,
      "learning_rate": 7.135869565217392e-05,
      "loss": 0.2398,
      "step": 4740
    },
    {
      "epoch": 12.907608695652174,
      "grad_norm": 3.3918545246124268,
      "learning_rate": 7.108695652173914e-05,
      "loss": 0.2339,
      "step": 4750
    },
    {
      "epoch": 12.934782608695652,
      "grad_norm": 7.659781455993652,
      "learning_rate": 7.081521739130436e-05,
      "loss": 0.208,
      "step": 4760
    },
    {
      "epoch": 12.96195652173913,
      "grad_norm": 1.7611188888549805,
      "learning_rate": 7.054347826086957e-05,
      "loss": 0.186,
      "step": 4770
    },
    {
      "epoch": 12.98913043478261,
      "grad_norm": 1.212941288948059,
      "learning_rate": 7.027173913043479e-05,
      "loss": 0.2078,
      "step": 4780
    },
    {
      "epoch": 13.016304347826088,
      "grad_norm": 1.580225944519043,
      "learning_rate": 7e-05,
      "loss": 0.1874,
      "step": 4790
    },
    {
      "epoch": 13.043478260869565,
      "grad_norm": 1.2616541385650635,
      "learning_rate": 6.972826086956521e-05,
      "loss": 0.1619,
      "step": 4800
    },
    {
      "epoch": 13.070652173913043,
      "grad_norm": 5.967649459838867,
      "learning_rate": 6.945652173913045e-05,
      "loss": 0.2543,
      "step": 4810
    },
    {
      "epoch": 13.097826086956522,
      "grad_norm": 2.8188414573669434,
      "learning_rate": 6.918478260869566e-05,
      "loss": 0.1808,
      "step": 4820
    },
    {
      "epoch": 13.125,
      "grad_norm": 3.5870656967163086,
      "learning_rate": 6.891304347826088e-05,
      "loss": 0.2394,
      "step": 4830
    },
    {
      "epoch": 13.152173913043478,
      "grad_norm": 1.6531853675842285,
      "learning_rate": 6.864130434782608e-05,
      "loss": 0.1972,
      "step": 4840
    },
    {
      "epoch": 13.179347826086957,
      "grad_norm": 2.169100284576416,
      "learning_rate": 6.83695652173913e-05,
      "loss": 0.2443,
      "step": 4850
    },
    {
      "epoch": 13.206521739130435,
      "grad_norm": 4.383741855621338,
      "learning_rate": 6.809782608695653e-05,
      "loss": 0.1517,
      "step": 4860
    },
    {
      "epoch": 13.233695652173912,
      "grad_norm": 2.7929298877716064,
      "learning_rate": 6.782608695652173e-05,
      "loss": 0.1948,
      "step": 4870
    },
    {
      "epoch": 13.26086956521739,
      "grad_norm": 1.0489866733551025,
      "learning_rate": 6.755434782608697e-05,
      "loss": 0.2078,
      "step": 4880
    },
    {
      "epoch": 13.28804347826087,
      "grad_norm": 1.664455771446228,
      "learning_rate": 6.728260869565217e-05,
      "loss": 0.2331,
      "step": 4890
    },
    {
      "epoch": 13.315217391304348,
      "grad_norm": 3.822244882583618,
      "learning_rate": 6.70108695652174e-05,
      "loss": 0.2224,
      "step": 4900
    },
    {
      "epoch": 13.342391304347826,
      "grad_norm": 1.0670325756072998,
      "learning_rate": 6.673913043478262e-05,
      "loss": 0.197,
      "step": 4910
    },
    {
      "epoch": 13.369565217391305,
      "grad_norm": 1.418443202972412,
      "learning_rate": 6.646739130434782e-05,
      "loss": 0.214,
      "step": 4920
    },
    {
      "epoch": 13.396739130434783,
      "grad_norm": 0.7198385000228882,
      "learning_rate": 6.619565217391304e-05,
      "loss": 0.1593,
      "step": 4930
    },
    {
      "epoch": 13.423913043478262,
      "grad_norm": 2.6676976680755615,
      "learning_rate": 6.592391304347827e-05,
      "loss": 0.1805,
      "step": 4940
    },
    {
      "epoch": 13.451086956521738,
      "grad_norm": 1.1735856533050537,
      "learning_rate": 6.565217391304349e-05,
      "loss": 0.2045,
      "step": 4950
    },
    {
      "epoch": 13.478260869565217,
      "grad_norm": 1.586053490638733,
      "learning_rate": 6.538043478260871e-05,
      "loss": 0.1827,
      "step": 4960
    },
    {
      "epoch": 13.505434782608695,
      "grad_norm": 1.315954566001892,
      "learning_rate": 6.510869565217391e-05,
      "loss": 0.1991,
      "step": 4970
    },
    {
      "epoch": 13.532608695652174,
      "grad_norm": 1.0821932554244995,
      "learning_rate": 6.483695652173913e-05,
      "loss": 0.2059,
      "step": 4980
    },
    {
      "epoch": 13.559782608695652,
      "grad_norm": 3.1673502922058105,
      "learning_rate": 6.456521739130436e-05,
      "loss": 0.2182,
      "step": 4990
    },
    {
      "epoch": 13.58695652173913,
      "grad_norm": 1.7307332754135132,
      "learning_rate": 6.429347826086956e-05,
      "loss": 0.1855,
      "step": 5000
    },
    {
      "epoch": 13.61413043478261,
      "grad_norm": 3.728205919265747,
      "learning_rate": 6.402173913043478e-05,
      "loss": 0.2114,
      "step": 5010
    },
    {
      "epoch": 13.641304347826086,
      "grad_norm": 2.4188344478607178,
      "learning_rate": 6.375e-05,
      "loss": 0.2427,
      "step": 5020
    },
    {
      "epoch": 13.668478260869565,
      "grad_norm": 1.5037428140640259,
      "learning_rate": 6.347826086956523e-05,
      "loss": 0.2001,
      "step": 5030
    },
    {
      "epoch": 13.695652173913043,
      "grad_norm": 1.970820665359497,
      "learning_rate": 6.320652173913045e-05,
      "loss": 0.192,
      "step": 5040
    },
    {
      "epoch": 13.722826086956522,
      "grad_norm": 4.18670654296875,
      "learning_rate": 6.293478260869565e-05,
      "loss": 0.1988,
      "step": 5050
    },
    {
      "epoch": 13.75,
      "grad_norm": 1.9383147954940796,
      "learning_rate": 6.266304347826087e-05,
      "loss": 0.2597,
      "step": 5060
    },
    {
      "epoch": 13.777173913043478,
      "grad_norm": 5.066593170166016,
      "learning_rate": 6.239130434782608e-05,
      "loss": 0.2158,
      "step": 5070
    },
    {
      "epoch": 13.804347826086957,
      "grad_norm": 1.11103093624115,
      "learning_rate": 6.21195652173913e-05,
      "loss": 0.197,
      "step": 5080
    },
    {
      "epoch": 13.831521739130435,
      "grad_norm": 0.9614455699920654,
      "learning_rate": 6.184782608695652e-05,
      "loss": 0.2317,
      "step": 5090
    },
    {
      "epoch": 13.858695652173914,
      "grad_norm": 1.3828792572021484,
      "learning_rate": 6.157608695652174e-05,
      "loss": 0.1954,
      "step": 5100
    },
    {
      "epoch": 13.88586956521739,
      "grad_norm": 2.2053611278533936,
      "learning_rate": 6.130434782608696e-05,
      "loss": 0.2337,
      "step": 5110
    },
    {
      "epoch": 13.91304347826087,
      "grad_norm": 1.4491547346115112,
      "learning_rate": 6.103260869565217e-05,
      "loss": 0.2197,
      "step": 5120
    },
    {
      "epoch": 13.940217391304348,
      "grad_norm": 18.08898162841797,
      "learning_rate": 6.076086956521739e-05,
      "loss": 0.2459,
      "step": 5130
    },
    {
      "epoch": 13.967391304347826,
      "grad_norm": 0.8541751503944397,
      "learning_rate": 6.0489130434782614e-05,
      "loss": 0.219,
      "step": 5140
    },
    {
      "epoch": 13.994565217391305,
      "grad_norm": 4.059837341308594,
      "learning_rate": 6.021739130434783e-05,
      "loss": 0.2037,
      "step": 5150
    },
    {
      "epoch": 14.021739130434783,
      "grad_norm": 3.2642979621887207,
      "learning_rate": 5.994565217391305e-05,
      "loss": 0.1818,
      "step": 5160
    },
    {
      "epoch": 14.048913043478262,
      "grad_norm": 1.1421191692352295,
      "learning_rate": 5.967391304347827e-05,
      "loss": 0.186,
      "step": 5170
    },
    {
      "epoch": 14.076086956521738,
      "grad_norm": 1.345381736755371,
      "learning_rate": 5.9402173913043476e-05,
      "loss": 0.1999,
      "step": 5180
    },
    {
      "epoch": 14.103260869565217,
      "grad_norm": 1.624812364578247,
      "learning_rate": 5.9130434782608704e-05,
      "loss": 0.1851,
      "step": 5190
    },
    {
      "epoch": 14.130434782608695,
      "grad_norm": 1.6718337535858154,
      "learning_rate": 5.885869565217391e-05,
      "loss": 0.1632,
      "step": 5200
    },
    {
      "epoch": 14.157608695652174,
      "grad_norm": 1.9013841152191162,
      "learning_rate": 5.858695652173913e-05,
      "loss": 0.1874,
      "step": 5210
    },
    {
      "epoch": 14.184782608695652,
      "grad_norm": 1.421287178993225,
      "learning_rate": 5.831521739130435e-05,
      "loss": 0.2004,
      "step": 5220
    },
    {
      "epoch": 14.21195652173913,
      "grad_norm": 1.5519428253173828,
      "learning_rate": 5.804347826086957e-05,
      "loss": 0.2029,
      "step": 5230
    },
    {
      "epoch": 14.23913043478261,
      "grad_norm": 2.584955930709839,
      "learning_rate": 5.777173913043479e-05,
      "loss": 0.2193,
      "step": 5240
    },
    {
      "epoch": 14.266304347826088,
      "grad_norm": 3.3906192779541016,
      "learning_rate": 5.7499999999999995e-05,
      "loss": 0.196,
      "step": 5250
    },
    {
      "epoch": 14.293478260869565,
      "grad_norm": 1.2147763967514038,
      "learning_rate": 5.722826086956522e-05,
      "loss": 0.1945,
      "step": 5260
    },
    {
      "epoch": 14.320652173913043,
      "grad_norm": 1.1547397375106812,
      "learning_rate": 5.695652173913044e-05,
      "loss": 0.1876,
      "step": 5270
    },
    {
      "epoch": 14.347826086956522,
      "grad_norm": 1.339676856994629,
      "learning_rate": 5.668478260869565e-05,
      "loss": 0.1647,
      "step": 5280
    },
    {
      "epoch": 14.375,
      "grad_norm": 1.1369940042495728,
      "learning_rate": 5.641304347826087e-05,
      "loss": 0.2113,
      "step": 5290
    },
    {
      "epoch": 14.402173913043478,
      "grad_norm": 2.4026777744293213,
      "learning_rate": 5.6141304347826085e-05,
      "loss": 0.1879,
      "step": 5300
    },
    {
      "epoch": 14.429347826086957,
      "grad_norm": 2.3310587406158447,
      "learning_rate": 5.5869565217391306e-05,
      "loss": 0.2052,
      "step": 5310
    },
    {
      "epoch": 14.456521739130435,
      "grad_norm": 7.154253959655762,
      "learning_rate": 5.559782608695653e-05,
      "loss": 0.2176,
      "step": 5320
    },
    {
      "epoch": 14.483695652173912,
      "grad_norm": 1.15971040725708,
      "learning_rate": 5.532608695652174e-05,
      "loss": 0.1965,
      "step": 5330
    },
    {
      "epoch": 14.51086956521739,
      "grad_norm": 1.930474877357483,
      "learning_rate": 5.505434782608696e-05,
      "loss": 0.1788,
      "step": 5340
    },
    {
      "epoch": 14.53804347826087,
      "grad_norm": 1.3449491262435913,
      "learning_rate": 5.478260869565217e-05,
      "loss": 0.2181,
      "step": 5350
    },
    {
      "epoch": 14.565217391304348,
      "grad_norm": 1.9273885488510132,
      "learning_rate": 5.451086956521739e-05,
      "loss": 0.1589,
      "step": 5360
    },
    {
      "epoch": 14.592391304347826,
      "grad_norm": 1.893212914466858,
      "learning_rate": 5.423913043478262e-05,
      "loss": 0.3009,
      "step": 5370
    },
    {
      "epoch": 14.619565217391305,
      "grad_norm": 8.782268524169922,
      "learning_rate": 5.3967391304347825e-05,
      "loss": 0.2138,
      "step": 5380
    },
    {
      "epoch": 14.646739130434783,
      "grad_norm": 2.973160982131958,
      "learning_rate": 5.3695652173913046e-05,
      "loss": 0.2276,
      "step": 5390
    },
    {
      "epoch": 14.673913043478262,
      "grad_norm": 1.4508061408996582,
      "learning_rate": 5.3423913043478266e-05,
      "loss": 0.2066,
      "step": 5400
    },
    {
      "epoch": 14.701086956521738,
      "grad_norm": 1.5062381029129028,
      "learning_rate": 5.315217391304348e-05,
      "loss": 0.1942,
      "step": 5410
    },
    {
      "epoch": 14.728260869565217,
      "grad_norm": 3.309744119644165,
      "learning_rate": 5.28804347826087e-05,
      "loss": 0.1755,
      "step": 5420
    },
    {
      "epoch": 14.755434782608695,
      "grad_norm": 1.350616693496704,
      "learning_rate": 5.260869565217391e-05,
      "loss": 0.2235,
      "step": 5430
    },
    {
      "epoch": 14.782608695652174,
      "grad_norm": 1.6678165197372437,
      "learning_rate": 5.2336956521739136e-05,
      "loss": 0.2023,
      "step": 5440
    },
    {
      "epoch": 14.809782608695652,
      "grad_norm": 5.028059005737305,
      "learning_rate": 5.206521739130436e-05,
      "loss": 0.1915,
      "step": 5450
    },
    {
      "epoch": 14.83695652173913,
      "grad_norm": 2.7195885181427,
      "learning_rate": 5.1793478260869564e-05,
      "loss": 0.2267,
      "step": 5460
    },
    {
      "epoch": 14.86413043478261,
      "grad_norm": 2.2020487785339355,
      "learning_rate": 5.1521739130434785e-05,
      "loss": 0.1794,
      "step": 5470
    },
    {
      "epoch": 14.891304347826086,
      "grad_norm": 1.3350173234939575,
      "learning_rate": 5.125e-05,
      "loss": 0.2031,
      "step": 5480
    },
    {
      "epoch": 14.918478260869565,
      "grad_norm": 1.5789434909820557,
      "learning_rate": 5.097826086956522e-05,
      "loss": 0.1835,
      "step": 5490
    },
    {
      "epoch": 14.945652173913043,
      "grad_norm": 1.4558829069137573,
      "learning_rate": 5.070652173913044e-05,
      "loss": 0.2084,
      "step": 5500
    },
    {
      "epoch": 14.972826086956522,
      "grad_norm": 2.362542152404785,
      "learning_rate": 5.0434782608695655e-05,
      "loss": 0.2023,
      "step": 5510
    },
    {
      "epoch": 15.0,
      "grad_norm": 1.2024067640304565,
      "learning_rate": 5.0163043478260875e-05,
      "loss": 0.2152,
      "step": 5520
    },
    {
      "epoch": 15.027173913043478,
      "grad_norm": 1.9680352210998535,
      "learning_rate": 4.989130434782609e-05,
      "loss": 0.1571,
      "step": 5530
    },
    {
      "epoch": 15.054347826086957,
      "grad_norm": 1.0427780151367188,
      "learning_rate": 4.9619565217391303e-05,
      "loss": 0.1775,
      "step": 5540
    },
    {
      "epoch": 15.081521739130435,
      "grad_norm": 1.4805899858474731,
      "learning_rate": 4.9347826086956524e-05,
      "loss": 0.1559,
      "step": 5550
    },
    {
      "epoch": 15.108695652173912,
      "grad_norm": 1.5507261753082275,
      "learning_rate": 4.9076086956521745e-05,
      "loss": 0.1973,
      "step": 5560
    },
    {
      "epoch": 15.13586956521739,
      "grad_norm": 2.1763224601745605,
      "learning_rate": 4.880434782608696e-05,
      "loss": 0.1836,
      "step": 5570
    },
    {
      "epoch": 15.16304347826087,
      "grad_norm": 1.7700446844100952,
      "learning_rate": 4.853260869565217e-05,
      "loss": 0.1823,
      "step": 5580
    },
    {
      "epoch": 15.190217391304348,
      "grad_norm": 1.4194791316986084,
      "learning_rate": 4.8260869565217394e-05,
      "loss": 0.2123,
      "step": 5590
    },
    {
      "epoch": 15.217391304347826,
      "grad_norm": 2.481651782989502,
      "learning_rate": 4.798913043478261e-05,
      "loss": 0.1882,
      "step": 5600
    },
    {
      "epoch": 15.244565217391305,
      "grad_norm": 1.922778844833374,
      "learning_rate": 4.771739130434783e-05,
      "loss": 0.1816,
      "step": 5610
    },
    {
      "epoch": 15.271739130434783,
      "grad_norm": 3.613877296447754,
      "learning_rate": 4.744565217391305e-05,
      "loss": 0.2004,
      "step": 5620
    },
    {
      "epoch": 15.298913043478262,
      "grad_norm": 3.2002601623535156,
      "learning_rate": 4.7173913043478264e-05,
      "loss": 0.1909,
      "step": 5630
    },
    {
      "epoch": 15.326086956521738,
      "grad_norm": 6.748310089111328,
      "learning_rate": 4.690217391304348e-05,
      "loss": 0.1913,
      "step": 5640
    },
    {
      "epoch": 15.353260869565217,
      "grad_norm": 5.549604892730713,
      "learning_rate": 4.66304347826087e-05,
      "loss": 0.2381,
      "step": 5650
    },
    {
      "epoch": 15.380434782608695,
      "grad_norm": 1.9017480611801147,
      "learning_rate": 4.635869565217392e-05,
      "loss": 0.2093,
      "step": 5660
    },
    {
      "epoch": 15.407608695652174,
      "grad_norm": 6.288442611694336,
      "learning_rate": 4.608695652173913e-05,
      "loss": 0.2251,
      "step": 5670
    },
    {
      "epoch": 15.434782608695652,
      "grad_norm": 3.496764898300171,
      "learning_rate": 4.581521739130435e-05,
      "loss": 0.2267,
      "step": 5680
    },
    {
      "epoch": 15.46195652173913,
      "grad_norm": 1.406836986541748,
      "learning_rate": 4.554347826086957e-05,
      "loss": 0.2264,
      "step": 5690
    },
    {
      "epoch": 15.48913043478261,
      "grad_norm": 1.726208209991455,
      "learning_rate": 4.527173913043479e-05,
      "loss": 0.1833,
      "step": 5700
    },
    {
      "epoch": 15.516304347826086,
      "grad_norm": 3.018139362335205,
      "learning_rate": 4.5e-05,
      "loss": 0.1734,
      "step": 5710
    },
    {
      "epoch": 15.543478260869565,
      "grad_norm": 1.2982267141342163,
      "learning_rate": 4.472826086956522e-05,
      "loss": 0.1539,
      "step": 5720
    },
    {
      "epoch": 15.570652173913043,
      "grad_norm": 2.031569242477417,
      "learning_rate": 4.445652173913044e-05,
      "loss": 0.1871,
      "step": 5730
    },
    {
      "epoch": 15.597826086956522,
      "grad_norm": 1.229080319404602,
      "learning_rate": 4.418478260869565e-05,
      "loss": 0.1641,
      "step": 5740
    },
    {
      "epoch": 15.625,
      "grad_norm": 2.0624427795410156,
      "learning_rate": 4.391304347826087e-05,
      "loss": 0.2027,
      "step": 5750
    },
    {
      "epoch": 15.652173913043478,
      "grad_norm": 2.0751490592956543,
      "learning_rate": 4.3641304347826087e-05,
      "loss": 0.1947,
      "step": 5760
    },
    {
      "epoch": 15.679347826086957,
      "grad_norm": 1.6509079933166504,
      "learning_rate": 4.336956521739131e-05,
      "loss": 0.1595,
      "step": 5770
    },
    {
      "epoch": 15.706521739130435,
      "grad_norm": 1.7256001234054565,
      "learning_rate": 4.309782608695652e-05,
      "loss": 0.1731,
      "step": 5780
    },
    {
      "epoch": 15.733695652173914,
      "grad_norm": 1.1034382581710815,
      "learning_rate": 4.282608695652174e-05,
      "loss": 0.255,
      "step": 5790
    },
    {
      "epoch": 15.76086956521739,
      "grad_norm": 1.832421064376831,
      "learning_rate": 4.255434782608696e-05,
      "loss": 0.2149,
      "step": 5800
    },
    {
      "epoch": 15.78804347826087,
      "grad_norm": 1.2617754936218262,
      "learning_rate": 4.228260869565218e-05,
      "loss": 0.203,
      "step": 5810
    },
    {
      "epoch": 15.815217391304348,
      "grad_norm": 1.1491669416427612,
      "learning_rate": 4.201086956521739e-05,
      "loss": 0.2072,
      "step": 5820
    },
    {
      "epoch": 15.842391304347826,
      "grad_norm": 2.8564293384552,
      "learning_rate": 4.1739130434782605e-05,
      "loss": 0.1802,
      "step": 5830
    },
    {
      "epoch": 15.869565217391305,
      "grad_norm": 1.2106716632843018,
      "learning_rate": 4.146739130434783e-05,
      "loss": 0.1619,
      "step": 5840
    },
    {
      "epoch": 15.896739130434783,
      "grad_norm": 1.6219168901443481,
      "learning_rate": 4.119565217391305e-05,
      "loss": 0.1726,
      "step": 5850
    },
    {
      "epoch": 15.923913043478262,
      "grad_norm": 4.405567646026611,
      "learning_rate": 4.092391304347826e-05,
      "loss": 0.212,
      "step": 5860
    },
    {
      "epoch": 15.951086956521738,
      "grad_norm": 4.760711669921875,
      "learning_rate": 4.065217391304348e-05,
      "loss": 0.2051,
      "step": 5870
    },
    {
      "epoch": 15.978260869565217,
      "grad_norm": 1.4360626935958862,
      "learning_rate": 4.0380434782608696e-05,
      "loss": 0.2139,
      "step": 5880
    },
    {
      "epoch": 16.005434782608695,
      "grad_norm": 1.444535493850708,
      "learning_rate": 4.0108695652173916e-05,
      "loss": 0.2099,
      "step": 5890
    },
    {
      "epoch": 16.032608695652176,
      "grad_norm": 1.4413769245147705,
      "learning_rate": 3.983695652173913e-05,
      "loss": 0.1523,
      "step": 5900
    },
    {
      "epoch": 16.059782608695652,
      "grad_norm": 1.2674199342727661,
      "learning_rate": 3.956521739130435e-05,
      "loss": 0.1889,
      "step": 5910
    },
    {
      "epoch": 16.08695652173913,
      "grad_norm": 1.7062088251113892,
      "learning_rate": 3.9293478260869565e-05,
      "loss": 0.1654,
      "step": 5920
    },
    {
      "epoch": 16.11413043478261,
      "grad_norm": 3.395096778869629,
      "learning_rate": 3.9021739130434786e-05,
      "loss": 0.2002,
      "step": 5930
    },
    {
      "epoch": 16.141304347826086,
      "grad_norm": 1.4144705533981323,
      "learning_rate": 3.875e-05,
      "loss": 0.2247,
      "step": 5940
    },
    {
      "epoch": 16.168478260869566,
      "grad_norm": 1.461371660232544,
      "learning_rate": 3.847826086956522e-05,
      "loss": 0.1997,
      "step": 5950
    },
    {
      "epoch": 16.195652173913043,
      "grad_norm": 2.219111204147339,
      "learning_rate": 3.8206521739130435e-05,
      "loss": 0.177,
      "step": 5960
    },
    {
      "epoch": 16.222826086956523,
      "grad_norm": 3.1004350185394287,
      "learning_rate": 3.793478260869565e-05,
      "loss": 0.1865,
      "step": 5970
    },
    {
      "epoch": 16.25,
      "grad_norm": 1.2384356260299683,
      "learning_rate": 3.7663043478260876e-05,
      "loss": 0.1535,
      "step": 5980
    },
    {
      "epoch": 16.277173913043477,
      "grad_norm": 3.8291006088256836,
      "learning_rate": 3.739130434782609e-05,
      "loss": 0.1872,
      "step": 5990
    },
    {
      "epoch": 16.304347826086957,
      "grad_norm": 1.3158376216888428,
      "learning_rate": 3.7119565217391304e-05,
      "loss": 0.1963,
      "step": 6000
    }
  ],
  "logging_steps": 10,
  "max_steps": 7360,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1743887293046784.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
